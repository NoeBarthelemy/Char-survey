---
title: "Char_survey_analysis"
author: 'Noé Barthelemy'
date: "28/07/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Library
```{r}
library(tidyverse)
library(readxl)
library(esquisse)
library(nnet)
```

This is empty but I needed a script to try the sharing process with Rowan.
```{r}
Survey_Dummy_Variables_Rowan_V1_NB_edits <- read_excel("~/Bureau/PHD/Ongoing_Projects/Economics/DATA_SURVEY/Survey Dummy Variables_Rowan_V1_SEP2021.xlsx")

# Keep that one stored as backup object 
# Original_from_Rowan_BACKUP <- Survey_Dummy_Variables_Rowan_V1_NB_edits
```
Create some dummy variables to re-create what Rowan did originally by hand on excel: 
--> The current position is sorted.

```{r}
Survey_Dummy_Variables_Rowan_V1_September2021 <- Survey_Dummy_Variables_Rowan_V1_NB_edits %>% 
  mutate(., Male = ifelse(Gender == "Male", 1, 0)) %>%
  mutate(., Female = ifelse(Gender == "Female", 1, 0)) %>% 
  mutate(., Employed_Dummy = ifelse(Employment_status == "Employed", 1, 0)) %>% 
  mutate(., Retired_Dummy = ifelse(Employment_status == "Retired", 1, 0)) %>% 
  mutate(., Student_Dummy = ifelse(Employment_status == "Full time student / apprenticeship", 1, 0)) %>% 
  mutate(., Employment_status = ifelse(Employment_status == "Full time student / apprenticeship", "Student", Employment_status)) %>% 
  mutate(., Undergrad_Dummy = ifelse(Highest_education_level == "University - undergraduate degree", 1, 0)) %>% 
  mutate(., Postgrad_Dummy = ifelse(Highest_education_level == "University - postgraduate degree", 1, 0)) %>% 
  mutate(., No_Papers_Dummy = ifelse(Publications_read == "0", 1, 0)) %>% 
  mutate(., Some_Papers_Dummy = ifelse(Publications_read %in% c("1-5", "6-10", "11-20"), 1, 0)) %>% 
  mutate(., Many_Papers_Dummy = ifelse(Publications_read == "21+", 1, 0)) %>% 
  # Now sort the lottery thing: 
  mutate(., Q1_Risk_Rating = ifelse(Risk_lottery_money_99 == "Option A", 0, 1)) %>% 
  mutate(., Q2_Risk_Rating = ifelse(Risk_lottery_money_90 == "Option A", 0, 2)) %>% 
  mutate(., Q3_Risk_Rating = ifelse(Risk_lottery_money_80 == "Option A", 0, 3)) %>% 
  mutate(., Q4_Risk_Rating = ifelse(Risk_lottery_money_70 == "Option A", 0, 4)) %>% 
  mutate(., Q5_Risk_Rating = ifelse(Risk_lottery_money_60 == "Option A", 0, 5)) %>% 
  mutate(., Q6_Risk_Rating = ifelse(Risk_lottery_money_50 == "Option A", 0, 6)) %>% 
  mutate(., Q7_Risk_Rating = ifelse(Risk_lottery_money_40 == "Option A", 0, 7)) %>% 
  mutate(., Q8_Risk_Rating = ifelse(Risk_lottery_money_30 == "Option A", 0, 8)) %>% 
  mutate(., Q9_Risk_Rating = ifelse(Risk_lottery_money_20 == "Option A", 0, 9)) %>% 
  mutate(., Q10_Risk_Rating = ifelse(Risk_lottery_money_10 == "Option A", 0, 10)) %>% 
  rowwise(.) %>% 
  mutate(., Average_Lotto_Risk_Rating = mean(c(Q1_Risk_Rating, Q2_Risk_Rating, Q3_Risk_Rating, Q4_Risk_Rating, Q5_Risk_Rating, Q6_Risk_Rating, Q7_Risk_Rating, Q8_Risk_Rating, Q9_Risk_Rating, Q10_Risk_Rating))) %>% 
  # Now sort the population lottery thing: 
  mutate(., Q1_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_99 == "Option A", 0, 1)) %>% 
  mutate(., Q2_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_90 == "Option A", 0, 2)) %>% 
  mutate(., Q3_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_80 == "Option A", 0, 3)) %>% 
  mutate(., Q4_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_70 == "Option A", 0, 4)) %>% 
  mutate(., Q5_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_60 == "Option A", 0, 5)) %>% 
  mutate(., Q6_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_50 == "Option A", 0, 6)) %>% 
  mutate(., Q7_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_40 == "Option A", 0, 7)) %>% 
  mutate(., Q8_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_30 == "Option A", 0, 8)) %>% 
  mutate(., Q9_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_20 == "Option A", 0, 9)) %>% 
  mutate(., Q10_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_10 == "Option A", 0, 10)) %>% 
  rowwise(.) %>% 
  mutate(., Average_Cons_Risk_Rating_ = mean(c(Q1_Conservation_Risk_Rating_, Q2_Conservation_Risk_Rating_, Q3_Conservation_Risk_Rating_, Q4_Conservation_Risk_Rating_, Q5_Conservation_Risk_Rating_, Q6_Conservation_Risk_Rating_, Q7_Conservation_Risk_Rating_, Q8_Conservation_Risk_Rating_, Q9_Conservation_Risk_Rating_, Q10_Conservation_Risk_Rating_)))

```

# Things to do 

Threshold 10% of dataset to make a variable significant in the regression. 

List of things to do : 
7) Manually (or with code) tidy the "Highest education level" variable. The "College" should be put as "Undergraduate", to reflect the higher education level that it is.

8) Discuss with George about the missing values and how to deal with them: Remove them, impute the mean observation, impute with a predictive model or create dummy variables to bypass the whole thing?  (Same thing for "Char conservation interest" and "Sufficiently protected").

For the true or false: ALSO, report in the text of the paper which answers were really common and interesting (eg. if people think that char are endangered).

9) For "Arctic char qualities", record the common answers and report it in the paper and discuss. 

11) Publications read: Keep the three dummies. Check correlation between that and the True or False score. 

Next meeting: 10/09/2021 

12) Got the CRRA into a code.

More info about the CRRA : http://library.lshtm.ac.uk/MSc_PHDC/2014-2015/108643.pdf 
*** 
What we did is replaced the "inf" value of CRRA by -2 for VERY risk loving people
AND by -2.5 for the people who never switched ! 
***

After meeting : 

13) Check if the respondents who allocated more to HR in round 1 than in Round 2 (weird!) actually anticipated more than others that the LR pops could be unique too !

14) Find ways to optimise the model !

15) Removing the extreme risk-loving values may re-normalise the distribution. 

The objective is to normalise the CRRA values to include them into a regression. 

16) What to do with the allocation variables

How can we fit the allocation variables into the regression ?
Do we want to in the first place ?




# Documentation

ABOUT NA'S in regression models: 
https://bookdown.org/egarpor/PM-UC3M/app-nas.html 

How to chose the right model : 
https://www.princeton.edu/~otorres/LogitR101.pdf 

Ordered probit models: 
https://cran.r-project.org/web/packages/oglmx/vignettes/oglmxVignette.pdf 

To get familiar with models and stuff :
https://www.econometrics-with-r.org/4-1-simple-linear-regression.html 

About multicollinearity :
https://www.analyticsvidhya.com/blog/2020/03/what-is-multicollinearity/


About SUR "Seamingly Unrelated Regression" :
https://en.wikipedia.org/wiki/Seemingly_unrelated_regressions 

About CRRA : 
EXPLAINING THE CHARACTERISTICS OF THE POWER (CRRA) UTILITY FAMILY : https://personal.eur.nl/wakker/pdfspubld/08.6powerut.pdf 

About utility functions: 
https://www.investopedia.com/ask/answers/072915/what-utility-function-and-how-it-calculated.asp 


# 1) Dataset filtering and formatting. 

## A) Filter completion level and data missingness

For now we filter anything below 50 percent. 
```{r}
# At this point I'm also going to add random names to the answers using the babynames dataset
# Add a unique ID to each answer
 
library(babynames)
data(babynames)

Baby_names_only <- babynames %>% select(., name) %>% unique()

# Commented line below so that the names don't change once sampled! 
# Baby_names_sample <- sample_n(Baby_names_only, nrow(Survey_Dummy_Variables_Rowan_V1_September2021)) 
Survey_Dummy_Variables_Rowan_V1_named<- cbind(Baby_names_sample, Survey_Dummy_Variables_Rowan_V1_September2021)

Survey_Dummy_Variables_Rowan_V2 <- Survey_Dummy_Variables_Rowan_V1_named %>% 
  filter(., Progress >= 70) %>% 
  # Create a variable that summarises missing data.
  mutate(MissingData = rowSums(is.na(.))) %>% 
  filter(., MissingData <= 20) %>% 
  select(., MissingData, everything()) 
```
apply(mtcars, MARGIN = 1, function(x) sum(is.na(x)))

## B) Sort the "Country" variable.

First we clean it. 
```{r}
Survey_Dummy_Variables_Rowan_V3 <- Survey_Dummy_Variables_Rowan_V2 %>% 
  ## We need one row per special case.
  mutate(., Country = if_else(condition = Country == "uk", true = "UK", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "GB", true = "UK", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "United Kingdom", true = "UK", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "ireland", true = "Ireland", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "Ireland.", true = "Ireland", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "canada", true = "Canada", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "germany", true = "Germany", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "N.Ireland", true = "Northern_Ireland", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "Nlreland", true = "Northern_Ireland", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "Northern Ireland", true = "Northern_Ireland", false = Country)) %>% 
  ## Special_case : No country entered. 
  mutate(., Country = if_else(condition = is.na(Country), true = "Unknown", false = Country)) %>% 
  select(., Country, everything())
```

Then we add variables that are going to summarise them in upper groups. 
```{r}
Survey_Dummy_Variables_Rowan_V4 <- Survey_Dummy_Variables_Rowan_V3 %>% 
  mutate(., Geographic_group = if_else(condition = Country %in% c("UK", "Ireland", "Northern_Ireland", "England", "Scotland", "Wales"), true = "North_Atlantic_Isles", false = if_else(condition = Country %in% c("Germany", "France", "Italy", "Norway", "Czech Republic", "Netherlands", "Sweden", "Denmark", "Finland", "Switzerland"), true = "Other_Europe", false = if_else(condition = Country %in% c("Canada", "USA"), true = "North_America", false = "WhereIsThat?" )))) %>% 
  select(., Country,Geographic_group, everything())
```

## C) Sort the "Year of Birth" variable.

```{r}
Survey_Dummy_Variables_Rowan_V5 <- Survey_Dummy_Variables_Rowan_V4 %>% 
  # Create the age variable
  mutate(., Age = 2021 - as.numeric(Year_of_birth)) %>% 
  # Group by class of age ! 
  mutate(., Age_grouped = if_else(Age <= 30, "EarlyAge", if_else(Age >= 31 & Age <= 60, "MediumAge", if_else(Age >= 61, "OldAge", "Problem!!")))) %>% 
  select(., Year_of_birth, Age,Age_grouped,  everything()) 
```

## D) Sort the "Current position" dummies.

```{r}
Survey_Dummy_Variables_Rowan_V6 <- Survey_Dummy_Variables_Rowan_V5 %>% 
  separate(., Current_position, into = c("Position1", "Position2", "Position3"), sep = ",") %>% 
  mutate(., University_Research_Dummy = if_else(condition = Position1 == "University (public) research" | Position2 == "University (public) research" | Position3 == "University (public) research", true = 1, false = 0, missing = 0)) %>%  
  mutate(., Environmental_Protection_Agency_Dummy = if_else(condition = Position1 == "Environmental protection agency" | Position2 == "Environmental protection agency" | Position3 == "Environmental protection agency", true = 1, false = 0, missing = 0)) %>%  
  mutate(., Wildlife_Conservation_Dummy = if_else(condition = Position1 == "Wildlife conservation/management agency" | Position2 == "Wildlife conservation/management agency" | Position3 == "Wildlife conservation/management agency", true = 1, false = 0, missing = 0)) %>% 
  mutate(., Other_position_Dummy = ifelse( University_Research_Dummy == 0 & Environmental_Protection_Agency_Dummy == 0 & Wildlife_Conservation_Dummy == 0, 1, 0 )) %>% 
  # It did the job, let's filter out those useless columns: 
  select(., -c("Position1", "Position2","Position3"))
  
```

## E) Sort the conservation experience variable

  Change the conservation experience variable, put them all in dummy variables. 
  Use gsub to remov the "Yes,".
  
```{r}
Survey_Dummy_Variables_Rowan_V7 <- Survey_Dummy_Variables_Rowan_V6 %>% 
  mutate(., Volunteering_Exp_Dummy = ifelse(grepl(pattern = "volunteering", x = Conservation_experience) == T, yes = "1",  no =  "0")) %>% 
  mutate(., Work_Exp_Dummy = ifelse(grepl(pattern = "work", x = Conservation_experience) == T, yes = "1",  no =  "0")) %>% 
  mutate(., Study_Exp_Dummy = ifelse(grepl(pattern = "studies", x = Conservation_experience) == T, yes = "1",  no =  "0")) 
```


## F) Sort the "I don't know"'s.

Missing values and how to deal with them: Remove them, impute the mean observation, impute with a predictive model or create dummy variables to bypass the whole thing?  (Same thing for "Char conservation interest" and "Sufficiently protected")

Let's inspect how much missing data we have in the first place !
```{r}
Survey_Dummy_Variables_Rowan_V7 %>% 
  count(., Confidence_in_survey == "I don't know")
```
```{r}
Survey_Dummy_Variables_Rowan_V7 %>% 
  count(., Char_conservation_interest == "I don't know")
```
```{r}
Survey_Dummy_Variables_Rowan_V7 %>% 
  count(., Scale_Sufficently_Protected_First == "I don't know")
```
So quite low numbers after all. Let's turn them into the mean observation! 

Calculate the mean of observations:
```{r}
Mean_Confidence_in_survey = mean(as.numeric(Survey_Dummy_Variables_Rowan_V7$Confidence_in_survey), na.rm = T)
Mean_Char_conservation_interest = mean(as.numeric(Survey_Dummy_Variables_Rowan_V7$Char_conservation_interest), na.rm = T)
Mean_Scale_Sufficently_Protected_First = mean(as.numeric(Survey_Dummy_Variables_Rowan_V7$Scale_Sufficently_Protected_First), na.rm = T)
Mean_Scale_Sufficently_Protected_Second = mean(as.numeric(Survey_Dummy_Variables_Rowan_V7$Scale_Sufficently_Protected_Second), na.rm = T)

Mean_Confidence_in_survey
Mean_Char_conservation_interest
Mean_Scale_Sufficently_Protected_First
```
Change the "I don't know" for the mean values
```{r}
Survey_Dummy_Variables_Rowan_V8 <- Survey_Dummy_Variables_Rowan_V7 %>% 
  mutate(., Confidence_in_survey = ifelse(Confidence_in_survey == "I don't know", yes = Mean_Confidence_in_survey, no = Confidence_in_survey)) %>% 
  mutate(., Char_conservation_interest = ifelse(Char_conservation_interest == "I don't know", yes = Mean_Char_conservation_interest, no = Char_conservation_interest)) %>% 
  mutate(., Scale_Sufficently_Protected_First = ifelse(Scale_Sufficently_Protected_First == "I don't know", yes = Mean_Scale_Sufficently_Protected_First, no = Scale_Sufficently_Protected_First)) %>% 
  mutate(., Scale_Sufficently_Protected_Second = ifelse(Scale_Sufficently_Protected_Second == "I don't know", yes = Mean_Scale_Sufficently_Protected_Second, no = Scale_Sufficently_Protected_Second))
  
```

## G) Sort the "True or False"

For the "True or false" : Use regex to create a dummy variable for each answer, then compute a score: +1 point for each true answer, -1 for each negative. ALSO, report in the text of the paper which answers were really common and interesting (eg. if people think that char are endangered).

It is relatively easy but takes a few lines of code:
We'll create score variables for each value found, and then sum up this into a global score variable.
Then we remove the score variables and keep only the global score.
```{r}
# We need a function to operate the sum at the end
p_sum <- function(...) {
  rowSums(pick(...))
}


Survey_Dummy_Variables_Rowan_V9 <- Survey_Dummy_Variables_Rowan_V8 %>% 
  # The true ones first: 
  mutate(., TrueFalse_DV_3 = ifelse(grepl(pattern = "oldest", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_5 = ifelse(grepl(pattern = "phenotypic", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_6 = ifelse(grepl(pattern = "different", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_7 = ifelse(grepl(pattern = "extinct", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_9 = ifelse(grepl(pattern = "sensitive", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_12= ifelse(grepl(pattern = "colorful", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  # Now the false one, minus one point: 
  mutate(., TrueFalse_DV_1 = ifelse(grepl(pattern = "farming", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_2 = ifelse(grepl(pattern = "IUCN", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_4 = ifelse(grepl(pattern = "Brown", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_8 = ifelse(grepl(pattern = "declined", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_10= ifelse(grepl(pattern = "Climate", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_11= ifelse(grepl(pattern = "Anadromous", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  # Make a score.
  mutate(., TrueFalseScore = as.numeric(TrueFalse_DV_1)+ as.numeric(TrueFalse_DV_2)+ as.numeric(TrueFalse_DV_3)+ as.numeric(TrueFalse_DV_4)+ as.numeric(TrueFalse_DV_5)+ as.numeric(TrueFalse_DV_6)+ as.numeric(TrueFalse_DV_7)+ as.numeric(TrueFalse_DV_8)+ as.numeric(TrueFalse_DV_9) +as.numeric(TrueFalse_DV_10)+ as.numeric(TrueFalse_DV_11) + as.numeric(TrueFalse_DV_12)) %>% 
  # Remove the crap.
  select(., -c(TrueFalse_DV_1, TrueFalse_DV_2, TrueFalse_DV_3, TrueFalse_DV_4, TrueFalse_DV_5, TrueFalse_DV_6, TrueFalse_DV_7,TrueFalse_DV_8,TrueFalse_DV_9,TrueFalse_DV_10,TrueFalse_DV_11,TrueFalse_DV_12))
```

## H) Calculate the "Lottery switch" and "CRRA" values

Basically, when are they switching from B to A ?

For the lines below we use that scale:

0.98	Highly risk averse
	
0.84	Very risk averse
	
0.67	Risk averse
	
0.48	Risk averse
	
0.26	Slightly risk averse
	
0	Risk neutral
	
-0.33	Risk loving
	
-0.74	Very risk loving
	
-1.33	Highly risk loving
	
inf	Highly risk loving

### I) Money CRRA
```{r}
Survey_Dummy_Variables_Rowan_V10 <- Survey_Dummy_Variables_Rowan_V9 %>% 
  mutate(., Money_Switch = ifelse(Risk_lottery_money_99 == "Option A", "99", ifelse(Risk_lottery_money_90 == "Option A", "90", ifelse(Risk_lottery_money_80 == "Option A", "80",ifelse(Risk_lottery_money_70 == "Option A", "70", ifelse(Risk_lottery_money_60 == "Option A", "60", ifelse(Risk_lottery_money_50 == "Option A", "50", ifelse(Risk_lottery_money_40 == "Option A", "40", ifelse(Risk_lottery_money_30 == "Option A", "30", ifelse(Risk_lottery_money_30 == "Option A", "30", ifelse(Risk_lottery_money_20 == "Option A", "20", ifelse(Risk_lottery_money_10 == "Option A", "10", "0"))) )))) ) )) )) %>% 
  
  # Get the CRRA into the code 
  mutate(., Money_CRRA = ifelse(Money_Switch == "99", 0.98, ifelse(Money_Switch == "90", 0.84, ifelse(Money_Switch == "80", 0.67, ifelse(Money_Switch == "70", 0.48, ifelse(Money_Switch == "60", 0.26, ifelse(Money_Switch == "50", 0, ifelse(Money_Switch == "40", -0.33, ifelse(Money_Switch == "30", -0.74, ifelse(Money_Switch == "20", -1.33, ifelse(Money_Switch == "10", -2, no = -2.5) )) ) ))))))) %>% 
  
  
  # Create a Risk-attitude variable 
  # Based on the CRRA values we attribute some risk attitudes. We may have to change the thresholds later !
  mutate(., Money_RiskAttitude = ifelse(Money_CRRA == 0.98, "Highly risk averse", ifelse(Money_CRRA == 0.84, "Very risk averse", ifelse(Money_CRRA == 0.67, "Risk averse", ifelse(Money_CRRA == 0.48, "Risk averse", ifelse(Money_CRRA == 0.26, "Slightly risk averse", ifelse(Money_CRRA == 0, "Risk neutral", ifelse(Money_CRRA == -0.33, "Risk loving", ifelse(Money_CRRA == -0.74, "Very risk loving", ifelse(Money_CRRA == -1.33, "Highly risk loving", ifelse(Money_CRRA < -1.33 , "Highly risk loving", "Problem")))))))))) )
```

### II) Population CRRA
```{r}
Survey_Dummy_Variables_Rowan_V11 <- Survey_Dummy_Variables_Rowan_V10 %>% 
  mutate(., Pop_Switch = ifelse(Risk_lottery_populations_99 == "Option A", "99", ifelse(Risk_lottery_populations_90 == "Option A", "90", ifelse(Risk_lottery_populations_80 == "Option A", "80",ifelse(Risk_lottery_populations_70 == "Option A", "70", ifelse(Risk_lottery_populations_60 == "Option A", "60", ifelse(Risk_lottery_populations_50 == "Option A", "50", ifelse(Risk_lottery_populations_40 == "Option A", "40", ifelse(Risk_lottery_populations_30 == "Option A", "30", ifelse(Risk_lottery_populations_30 == "Option A", "30", ifelse(Risk_lottery_populations_20 == "Option A", "20", ifelse(Risk_lottery_populations_10 == "Option A", "10", "0"))) )))) ) )) )) %>% 
  
  # Get the CRRA into the code 
  mutate(., Pop_CRRA = ifelse(Pop_Switch == "99", 0.98, ifelse(Pop_Switch == "90", 0.84, ifelse(Pop_Switch == "80", 0.67, ifelse(Pop_Switch == "70", 0.48, ifelse(Pop_Switch == "60", 0.26, ifelse(Pop_Switch == "50", 0, ifelse(Pop_Switch == "40", -0.33, ifelse(Pop_Switch == "30", -0.74, ifelse(Pop_Switch == "20", -1.33, ifelse(Pop_Switch == "10", -2, no = -2.5) )) ) ))))))) %>% 
  
  
  # Create a Risk-attitude variable 
  # Based on the CRRA values we attribute some risk attitudes. We may have to change the thresholds later !
  mutate(., Pop_RiskAttitude = ifelse(Pop_CRRA == 0.98, "Highly risk averse", ifelse(Pop_CRRA == 0.84, "Very risk averse", ifelse(Pop_CRRA == 0.67, "Risk averse", ifelse(Pop_CRRA == 0.48, "Risk averse", ifelse(Pop_CRRA == 0.26, "Slightly risk averse", ifelse(Pop_CRRA == 0, "Risk neutral", ifelse(Pop_CRRA == -0.33, "Risk loving", ifelse(Pop_CRRA == -0.74, "Very risk loving", ifelse(Pop_CRRA == -1.33, "Highly risk loving", ifelse(Pop_CRRA < -1.33 , "Highly risk loving", "Problem")))))))))) )
```



## I) Sort the allocation budget variables

This is the new information given before Round 2: 

Initial round = All pops are of equal conservation status.
The 10 HR populations contains some unique genetic lineages.
Extinction of these = irremediable loss of genetic diversity of the oldest native fish species.

The interpretation is (From High Risk prospective): 
If from Round 1 to Round 2 allocation to HR goes UP : The respondent thought the HR were more valuable in the light of the new info (which highlights the uniqueness and the risk of huge loss). 
--> This is the most expected behavior.

If from Round 1 to Round 2 allocation to HR goes DOWN : Respondent thought the HR were not that valuable anymore. 
--> This is the less expected behavior. Maybe correlated to anticipation that LR can be unique too? 

If from Round 1 to Round 2 allocation to HR stays SAME : Respondent is unaffected by the info. 
--> Not expected. Either lack of understanding (test if correlated with duration) or maybe anticipation that LR can be unique too FROM THE START ! 
--> Unique case: If it stays the same at zero, could be that respondent attributes no value to the information compared to the fact that more populations can be saved by protecting the LR.

```{r}
Survey_Dummy_Variables_Rowan_V12 <- Survey_Dummy_Variables_Rowan_V11 %>% 
  
  # Create a nicer variable for Round 1:
  mutate(Alloc_LR_Round1 = ifelse(is.na(Risk_Reduction_LR_14_percent) & is.na(Risk_Reduction_LR_20_percent), NA, ifelse(is.na(Risk_Reduction_LR_20_percent), Risk_Reduction_LR_14_percent, ifelse(is.na(Risk_Reduction_LR_14_percent), Risk_Reduction_LR_20_percent, "Problem")))) %>% 
  mutate(Alloc_HR_Round1 = ifelse(is.na(Risk_Reduction_HR_50_percent) & is.na(Risk_Reduction_HR_70_percent), NA, ifelse(is.na(Risk_Reduction_HR_70_percent), Risk_Reduction_HR_50_percent, ifelse(is.na(Risk_Reduction_HR_50_percent), Risk_Reduction_HR_70_percent, "Problem")))) %>% 
  # Create a nicer variable for Round 2:
  mutate(Alloc_LR_Round2 = Risk_Reduction_LR_40_percent_Round_2) %>% 
  mutate(Alloc_HR_Round2 = Risk_Reduction_HR_80_percent_Round_2) %>% 
  # Create a nicer variable for Round 3:
  mutate(Alloc_LR_Round3 = Risk_Reduction_LR_40_percent_Round_3) %>% 
  mutate(Alloc_HR_Round3 = Risk_Reduction_HR_80_percent_Round_3) %>% 
  
  # Create a variable that summarises the allocation profile of a respondent: 
  # The idea is : Does the allocation goes up, or down? And then, up, down, the same?
  # To do that we need to put the focus on either the low risk or High risk.
  # I chose the High Risk allocation.
  mutate(., Alloc_profile_Round1to2 = ifelse(as.numeric(Alloc_HR_Round1) > as.numeric(Alloc_HR_Round2), "Down", ifelse(Alloc_HR_Round1 < Alloc_HR_Round2, "Up", ifelse(Alloc_HR_Round1 == Alloc_HR_Round2, "Same", "Problem")))) %>% 
  mutate(., Alloc_profile_Round2to3 = ifelse(Alloc_HR_Round2 > Alloc_HR_Round3, "Down", ifelse(Alloc_HR_Round2 < Alloc_HR_Round3, "Up", ifelse(Alloc_HR_Round2 == Alloc_HR_Round3, "Same", "Problem")))) %>% 

  unite("Alloc_profile_AllRounds", Alloc_profile_Round1to2, Alloc_profile_Round2to3, sep = "-", remove = F)  %>%   
  
  # Quantify this profile 
  mutate(., Alloc_diff_Round1to2 = as.numeric(Alloc_HR_Round2) - as.numeric(Alloc_HR_Round1)) %>% 
  mutate(., Alloc_diff_Round2to3 = as.numeric(Alloc_HR_Round3) - as.numeric(Alloc_HR_Round2))

```

```{r}
# Prepare some data for a plot : 
Allocation_diff_Round1to2 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(Alloc_diff_Round1to2) %>% 
  rename(., Alloc_diff = "Alloc_diff_Round1to2") %>% 
  mutate(., Round = "One to two")
Allocation_diff_Round2to3 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(Alloc_diff_Round2to3) %>% 
  rename(., Alloc_diff = "Alloc_diff_Round2to3") %>% 
  mutate(., Round = "Two to three")

Allocation_differences <- rbind(Allocation_diff_Round1to2, Allocation_diff_Round2to3)

# Now a nice density plot
ggplot(Allocation_differences, aes(x = Alloc_diff, fill = Round)) + geom_density(alpha = 0.5) +
 labs(x = "Difference of budget allocation to High-Risk populations between Rounds", y = "Density", title = "Niceplot") +
 theme_minimal()
```
```{r}
# Another plot to see the evolution in budget allocation 
Allocation_budgets1 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(., Alloc_HR_Round1, Alloc_LR_Round1, name) %>% 
  mutate(., Round = 1) %>% 
  rename(., Budget_alloc_HR = "Alloc_HR_Round1") %>% 
  rename(., Budget_alloc_LR = "Alloc_LR_Round1") 
Allocation_budgets2 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(., Alloc_HR_Round2, Alloc_LR_Round2, name)%>% 
  mutate(., Round = 2) %>% 
  rename(., Budget_alloc_HR = "Alloc_HR_Round2") %>% 
  rename(., Budget_alloc_LR = "Alloc_LR_Round2") 
Allocation_budgets3 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(., Alloc_HR_Round3, Alloc_LR_Round3, name)%>% 
  mutate(., Round = 3) %>% 
  rename(., Budget_alloc_HR = "Alloc_HR_Round3") %>% 
  rename(., Budget_alloc_LR = "Alloc_LR_Round3") 


Allocation_budgets <- rbind(Allocation_budgets1, Allocation_budgets2, Allocation_budgets3)
Allocation_budgets$Budget_alloc_HR <- as.numeric(Allocation_budgets$Budget_alloc_HR)
Allocation_budgets$Budget_alloc_LR <- as.numeric(Allocation_budgets$Budget_alloc_LR)


```
# NEED HELP TO PLOT THAT

```{r}
mean(Survey_Dummy_Variables_Rowan_V12$Alloc_diff_Round1to2, na.rm = T)
mean(Survey_Dummy_Variables_Rowan_V12$Alloc_diff_Round2to3, na.rm = T)
mean(as.numeric(Allocation_budgets1$Budget_alloc_HR), na.rm = T)
mean(as.numeric(Allocation_budgets1$Budget_alloc_LR), na.rm = T)
mean(Allocation_budgets2$Budget_alloc_HR, na.rm = T)
mean(Allocation_budgets2$Budget_alloc_LR, na.rm = T)
mean(Allocation_budgets3$Budget_alloc_HR, na.rm = T)
mean(Allocation_budgets3$Budget_alloc_LR, na.rm = T)
```
## J) Sort the "did you feel comfortable" variable 

When the people didn't check Yes or No and commented I assumed that it was a "YES".

```{r}
Survey_Dummy_Variables_Rowan_V13 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  rename(., "Comfortable_with_Exp" = `Did_you_feel_comfortable_with_the_budget_allocation_experiment?_-_Selected_Choice`) %>% 
  mutate(., Comfortable_with_Exp = ifelse(Comfortable_with_Exp == "No", "No", ifelse(Comfortable_with_Exp == "Yes", "Yes", ifelse(Comfortable_with_Exp == "No,Would you like to comment it so we can improve it ?", "No", ifelse(Comfortable_with_Exp == "Yes,Would you like to comment it so we can improve it ?", "Yes",  ifelse(is.na(Comfortable_with_Exp), NA, "Yes")))) )) 
```

## K) Sort the Duration variable

Ok we will not only put that in minutes but also take in account that some stupid respondents left the tab open for ages and thus don't fall AT ALL in the usual duration distribution.

```{r}
Survey_Dummy_Variables_Rowan_V14 <- Survey_Dummy_Variables_Rowan_V13 %>% 
  mutate(., Duration_in_minutes = Duration/60) 

summary(Survey_Dummy_Variables_Rowan_V14$Duration_in_minutes)
```
We will modify extreme values to a maximum cap of 50mn, which is way above the third quantile of the distribution. 
Nobody should really take more than 50 minutes. 

No actually it is better to put NA instead !!

```{r}
Survey_Dummy_Variables_Rowan_V14.5 <- Survey_Dummy_Variables_Rowan_V14 %>% 
  mutate(., Duration_in_minutes_capped = ifelse(Duration_in_minutes > 50, NA, Duration_in_minutes)) 
```


## L) Create a numeric version of papers read

```{r}
Survey_Dummy_Variables_Rowan_V15 <- Survey_Dummy_Variables_Rowan_V14.5 %>% 
  mutate(., Papers_read_fakeNumeric = ifelse(Publications_read == "0", 0, ifelse(Publications_read == "1-5", 5, ifelse(Publications_read == "6-10", 10, ifelse(Publications_read == "11-20", 20, ifelse(Publications_read == "21+", 30, NA)))) )) 
```


## M) Simple version 

Make a Simple version of the table
```{r}
Survey_Dummy_Variables_Rowan_Simple_V1 <- Survey_Dummy_Variables_Rowan_V15 %>% 
  select(., - c(Duration, Duration_in_minutes, Year_of_birth, Female, Male, Age ,Highest_education_level, Conservation_experience, Useless, True_false_question, Risk_lottery_money_99, Risk_lottery_money_90, Risk_lottery_money_80, Risk_lottery_money_70, Risk_lottery_money_60, Risk_lottery_money_50, Risk_lottery_money_40, Risk_lottery_money_30, Risk_lottery_money_20, Risk_lottery_money_10, Risk_lottery_populations_99, Risk_lottery_populations_90, Risk_lottery_populations_80, Risk_lottery_populations_70, Risk_lottery_populations_60, Risk_lottery_populations_50, Risk_lottery_populations_40, Risk_lottery_populations_30, Risk_lottery_populations_20, Risk_lottery_populations_10, Q1_Risk_Rating, Q2_Risk_Rating, Q3_Risk_Rating, Q4_Risk_Rating, Q5_Risk_Rating, Q6_Risk_Rating, Q7_Risk_Rating, Q8_Risk_Rating, Q9_Risk_Rating, Q10_Risk_Rating, Q1_Conservation_Risk_Rating_, Q2_Conservation_Risk_Rating_, Q3_Conservation_Risk_Rating_, Q4_Conservation_Risk_Rating_, Q5_Conservation_Risk_Rating_, Q6_Conservation_Risk_Rating_, Q7_Conservation_Risk_Rating_, Q8_Conservation_Risk_Rating_, Q9_Conservation_Risk_Rating_, Q10_Conservation_Risk_Rating_, Risk_Reduction_LR_14_percent, Risk_Reduction_LR_20_percent, Risk_Reduction_HR_70_percent, Risk_Reduction_HR_50_percent, Risk_Reduction_LR_40_percent_Round_2, Risk_Reduction_HR_80_percent_Round_2, Risk_Reduction_LR_40_percent_Round_3, Risk_Reduction_HR_80_percent_Round_3))  %>% 
  
  # FILTER OUT MORE NA'S TO SOLVE THE MISSING DATA PROBLEM
filter(., !is.na(Alloc_diff_Round2to3)) %>% 
  
  # Remove the Student / Employed / Retired dummies: They create false collinearity. 
select(., -c(Employed_Dummy, Student_Dummy, Retired_Dummy))
  

Survey_Dummy_Variables_Rowan_Simple_V1$Work_Exp_Dummy <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Work_Exp_Dummy)
Survey_Dummy_Variables_Rowan_Simple_V1$Study_Exp_Dummy <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Study_Exp_Dummy)
Survey_Dummy_Variables_Rowan_Simple_V1$Volunteering_Exp_Dummy <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Volunteering_Exp_Dummy)
Survey_Dummy_Variables_Rowan_Simple_V1$Confidence_in_survey <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Confidence_in_survey)
Survey_Dummy_Variables_Rowan_Simple_V1$Char_conservation_interest <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Char_conservation_interest)
Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_First <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_First)
Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_Second <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_Second)
Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_HR_Round1 <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_HR_Round1)
Survey_Dummy_Variables_Rowan_Simple_V1$Money_Switch <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Money_Switch)
Survey_Dummy_Variables_Rowan_Simple_V1$Pop_Switch <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Pop_Switch)
Survey_Dummy_Variables_Rowan_Simple_V1$Pop_CRRA <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Pop_CRRA)
Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_diff_Round2to3 <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_diff_Round2to3)
```

```{r}
# summary(Survey_Dummy_Variables_Rowan_Simple_V1)
```

# 2) Plotting

```{r}
# esquisser(Survey_Dummy_Variables_Rowan_Simple_V1)
```


# 3) The main models


## Preamble - How to interpret a model

1) Look at the Residuals

The next item in the model output talks about the residuals. Residuals are essentially the difference between the actual observed response values (distance to stop dist in our case) and the response values that the model predicted. The Residuals section of the model output breaks it down into 5 summary points. When assessing how well the model fit the data, you should look for a symmetrical distribution across these points on the mean value zero (0). In our example, we can see that the distribution of the residuals do not appear to be strongly symmetrical. That means that the model predicts certain points that fall far away from the actual observed points. We could take this further consider plotting the residuals to see whether this normally distributed, etc. but will skip this for this example.

2) Coefficients

The next section in the model output talks about the coefficients of the model. Theoretically, in simple linear regression, the coefficients are two unknown constants that represent the intercept and slope terms in the linear model. If we wanted to predict the Distance required for a car to stop given its speed, we would get a training set and produce estimates of the coefficients to then use it in the model formula. Ultimately, the analyst wants to find an intercept and a slope such that the resulting fitted line is as close as possible to the 50 data points in our data set.

Coefficient - Estimate

The coefficient Estimate contains two rows; the first one is the intercept. The intercept, in our example, is essentially the expected value of the distance required for a car to stop when we consider the average speed of all cars in the dataset. In other words, it takes an average car in our dataset 42.98 feet to come to a stop. The second row in the Coefficients is the slope, or in our example, the effect speed has in distance required for a car to stop. The slope term in our model is saying that for every 1 mph increase in the speed of a car, the required distance to stop goes up by 3.9324088 feet.

Coefficient - Standard Error

The coefficient Standard Error measures the average amount that the coefficient estimates vary from the actual average value of our response variable. We’d ideally want a lower number relative to its coefficients. In our example, we’ve previously determined that for every 1 mph increase in the speed of a car, the required distance to stop goes up by 3.9324088 feet. The Standard Error can be used to compute an estimate of the expected difference in case we ran the model again and again. In other words, we can say that the required distance for a car to stop can vary by 0.4155128 feet. The Standard Errors can also be used to compute confidence intervals and to statistically test the hypothesis of the existence of a relationship between speed and distance required to stop.

Coefficient - t value

The coefficient t-value is a measure of how many standard deviations our coefficient estimate is far away from 0. We want it to be far away from zero as this would indicate we could reject the null hypothesis - that is, we could declare a relationship between speed and distance exist. In our example, the t-statistic values are relatively far away from zero and are large relative to the standard error, which could indicate a relationship exists. In general, t-values are also used to compute p-values.

Coefficient - Pr(>t)

The Pr(>t) acronym found in the model output relates to the probability of observing any value equal or larger than t. A small p-value indicates that it is unlikely we will observe a relationship between the predictor (speed) and response (dist) variables due to chance. Typically, a p-value of 5% or less is a good cut-off point. In our model example, the p-values are very close to zero. Note the ‘signif. Codes’ associated to each estimate. Three stars (or asterisks) represent a highly significant p-value. Consequently, a small p-value for the intercept and the slope indicates that we can reject the null hypothesis which allows us to conclude that there is a relationship between speed and distance.

Residual Standard Error

Residual Standard Error is measure of the quality of a linear regression fit. Theoretically, every linear model is assumed to contain an error term E. Due to the presence of this error term, we are not capable of perfectly predicting our response variable (dist) from the predictor (speed) one. The Residual Standard Error is the average amount that the response (dist) will deviate from the true regression line. In our example, the actual distance required to stop can deviate from the true regression line by approximately 15.3795867 feet, on average. In other words, given that the mean distance for all cars to stop is 42.98 and that the Residual Standard Error is 15.3795867, we can say that the percentage error is (any prediction would still be off by) 35.78%. It’s also worth noting that the Residual Standard Error was calculated with 48 degrees of freedom. Simplistically, degrees of freedom are the number of data points that went into the estimation of the parameters used after taking into account these parameters (restriction). In our case, we had 50 data points and two parameters (intercept and slope).

Multiple R-squared, Adjusted R-squared

The R-squared (R2
) statistic provides a measure of how well the model is fitting the actual data. It takes the form of a proportion of variance. R2 is a measure of the linear relationship between our predictor variable (speed) and our response / target variable (dist). It always lies between 0 and 1 (i.e.: a number near 0 represents a regression that does not explain the variance in the response variable well and a number close to 1 does explain the observed variance in the response variable). In our example, the R2 we get is 0.6510794. Or roughly 65% of the variance found in the response variable (dist) can be explained by the predictor variable (speed). Step back and think: If you were able to choose any metric to predict distance required for a car to stop, would speed be one and would it be an important one that could help explain how distance would vary based on speed? I guess it’s easy to see that the answer would almost certainly be a yes. That why we get a relatively strong R2. Nevertheless, it’s hard to define what level of R2

is appropriate to claim the model fits well. Essentially, it will vary with the application and the domain studied.

A side note: In multiple regression settings, the R2
will always increase as more variables are included in the model. That’s why the adjusted R2

is the preferred measure as it adjusts for the number of variables considered.

F-Statistic

F-statistic is a good indicator of whether there is a relationship between our predictor and the response variables. The further the F-statistic is from 1 the better it is. However, how much larger the F-statistic needs to be depends on both the number of data points and the number of predictors. Generally, when the number of data points is large, an F-statistic that is only a little bit larger than 1 is already sufficient to reject the null hypothesis (H0 : There is no relationship between speed and distance). The reverse is true as if the number of data points is small, a large F-statistic is required to be able to ascertain that there may be a relationship between predictor and response variables. In our example the F-statistic is 89.5671065 which is relatively larger than 1 given the size of our data.




In the regression we include : 

Gender, Birth, Country, Employment (Use employment VS study, use the dummy variables), Current position (University_DV + EPA_DV + Wildlife_DV), Highest education (Use only the dummies "Undergradute" and "Postgraduate"), Conservation experience (Use all the three as dummy variables), Confidence in Survey (once "I don't know" values are sorted), Char conservation interest, True or false score, Publications read (Use the three dummies), . 


## I) Model Round 1) 

These budget allocation models will use the allocation amount to HR populations as a response variable, 
and all the remaining variables as explanatory.

We will try this Round per Round first and see what we get.

+ = I took this one into the model

 [1] "Age_grouped"                            +                                                     
 [2] "Country"                                                                          
 [3] "Geographic_group"                       +                                                    
 [4] "name"                                                                             
 [5] "Progress"                                                                         
 [6] "Duration"                               +                                                    
 [7] "Finished"                                                                         
 [8] "Gender"                                 +                                                    
 [9] "Male"                                                                                       
[10] "Female"                                                                           
[11] "Employed_Dummy"                         +                                                    
[12] "Student_Dummy"                          +                                                    
[13] "Retired_Dummy"                          +                                                    
[14] "University_Research_Dummy"              +                                          
[15] "Environmental_Protection_Agency_Dummy"  +                                          
[16] "Wildlife_Conservation_Dummy"            +                                          
[17] "Current_position_other"                                                           
[18] "Undergrad_Dummy"                        +                                          
[19] "Postgrad_Dummy"                         +                                          
[20] "Highest_education_level_other"                                                    
[21] "Work_Exp_Dummy"                         +                                          
[22] "Study_Exp_Dummy"                        +                                          
[23] "Volunteering_Exp_Dummy"                 +                                          
[24] "Confidence_in_survey"                   +                                          
[25] "Char_conservation_interest"             +                                          
[26] "Arctic_char_qualities"                                                            
[27] "No_Papers_Dummy"                        +                                          
[28] "Some_Papers_Dummy"                      +                                          
[29] "Many_Papers_Dummy"                      +                                          
[30] "Scale_Sufficently_Protected_First"      +                                          
[31] "Average_Lotto_Risk_Rating"                                                        
[32] "Average_Cons_Risk_Rating_"                                                        
[33] "Information_anticipation"                                                         
[34] "Did_you_feel_comfortable_with_
the_budget_allocation_experiment?_-_Selected_Choice"
[35] "Comfortable_with_experiment"                                                      
[36] "Scale_Sufficently_Protected_Second"     +                                          
[37] "Taxonomy"                                                                         
[38] "Survey_comments"                                                                  
[39] "Group"                                  +                                          
[40] "TrueFalseScore"                         +                                          
[41] "Money_Switch"                           +                                          
[42] "Money_CRRA"                             +                                          
[43] "Pop_Switch"                             +                                          
[44] "Pop_CRRA"                               +                                          
[45] "Alloc_LR_Round1"                                                                  
[46] "Alloc_HR_Round1"                                                                  
[47] "Alloc_LR_Round2"                                                                  
[48] "Alloc_HR_Round2"                        +                                          
[49] "Alloc_LR_Round3"                                                                  
[50] "Alloc_HR_Round3"                        +                                          
[51] "Alloc_profile_AllRounds"                +                                          
[52] "Alloc_profile_Round1to2"                                                          
[53] "Alloc_profile_Round2to3"                                                          
[54] "Alloc_diff_Round1to2"                                                             
[55] "Alloc_diff_Round2to3"   

```{r}
Linearmod_test_Round1 <- lm(formula = Alloc_HR_Round1 ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          # Money_Switch + Collinear !!
                          Money_CRRA +
                          # Pop_Switch +  Collinear !!
                          Pop_CRRA + 
                          Alloc_HR_Round2 +
                          Alloc_HR_Round3 +
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```
  
### a) Multicollinearity 

VIF (Variable Inflation Factors).

 "VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable." 
 
 * Interpretation guide *
    VIF starts at 1 and has no upper limit
    VIF = 1, no correlation between the independent variable and the other variables
    VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others


```{r}
alias( Linearmod_test_Round1 )

car::vif(Linearmod_test_Round1)
```
  
  
```{r}
summary(Linearmod_test_Round1)
```
   

## II) Model Round 2)

```{r}
Linearmod_test_Round2 <- lm(formula = Alloc_HR_Round2 ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round3 +
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```
  
```{r}
summary(Linearmod_test_Round2)
```

## III) Model Round 3) 

```{r}
Linearmod_test_Round3 <- lm(formula = Alloc_HR_Round3 ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round2 +
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```
  
```{r}
summary(Linearmod_test_Round3)
```

## IV) Multinomial logistic reg: Model on profile

Based on profile like Up-Down, Down-Down etc.
Since the response variable is categorical we need a multinomial logistic regression.

https://datasciencebeginners.com/2018/12/20/multinomial-logistic-regression-using-r/

```{r}
# Setting the basline 
Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_profile_AllRounds <- relevel(as.factor(Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_profile_AllRounds), ref = "Same-Same")

Linearmod_test_Profile <- multinom(formula = Alloc_profile_AllRounds ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round2 +
                          Alloc_HR_Round3 +
                          Information_anticipation 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```

# NO CLUE OF HOW TO INTERPRET THIS
```{r}
summary(Linearmod_test_Profile)
```
## V) Model Diff Round 1 to 2 

Now the goal is to make a model that predicts the difference of allocation between Round 1 and Round 2. 
This should be interesting. 
Then we will do the same with Round 2 to 3. 
I think that a linear regression will do the job!

```{r}
Linearmod_Diff_Round1to2 <- lm(formula = Alloc_diff_Round1to2 ~ 
                          Age_grouped +
                          Geographic_group +
                          Duration_in_minutes_capped +
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round3 +
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1,
                        singular.ok = F) 
```

### a) Multi-collinearity ?

```{r}
alias( Linearmod_Diff_Round1to2 )

car::vif(Linearmod_Diff_Round1to2)
```


```{r}
summary(Linearmod_Diff_Round1to2)
```

## VI) Model Diff Round 2 to 3

We do the same with Round 2 to 3. 

```{r}
Linearmod_Diff_Round2to3 <- lm(formula = Alloc_diff_Round2to3 ~ 
                          Age_grouped +
                          Geographic_group +
                          Duration_in_minutes_capped +
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                        , data = as.data.frame(Survey_Dummy_Variables_Rowan_Simple_V1)) 
```
  
```{r}
alias( Linearmod_Diff_Round2to3 )

car::vif(Linearmod_Diff_Round2to3)
```
  
```{r}
summary(Linearmod_Diff_Round2to3)
```

## VII) Model CRRA populations

We do the same with the CRRA with populations to detect issues. 
Removed the Pop and Money switches to avoid collinearity

```{r}
Linearmod_Pop_CRRA <- lm(formula = Pop_CRRA ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Information_anticipation 
                        
                        , data = as.data.frame(Survey_Dummy_Variables_Rowan_Simple_V1)) 
```
  
```{r}
summary(Linearmod_Pop_CRRA)
```

## VIII) Model CRRA populations - NoMoney questions

We do the same with the CRRA with populations to detect issues. 

```{r}
Linearmod_NoMoney_Pop_CRRA <- lm(formula = Pop_CRRA ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Information_anticipation 
                        
                        , data = as.data.frame(Survey_Dummy_Variables_Rowan_Simple_V1)) 
```

Our data is not normally distributed.
```{r}
ggplot(Survey_Dummy_Variables_Rowan_Simple_V1) +
 aes(x = Money_CRRA) +
 geom_histogram(bins = 30L, fill = "#112446") +
 theme_minimal()
```
Check multicollinearity 
```{r}
alias( Linearmod_NoMoney_Pop_CRRA )

car::vif(Linearmod_NoMoney_Pop_CRRA)
```


  
```{r}
summary(Linearmod_NoMoney_Pop_CRRA)
```
## IX) Normalise data and create new model with integrated difference in risk aversions. 

The idea is to combine our CRRA and Allocation variables in order to see if they give more information within a regression model. Let's try to see how we can combine these things in a meaningful way.

The objective is to normalize the CRRA values to include them into a regression.

```{r}
Survey_Dummy_Variables_Rowan_Simple_V2 <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  mutate(., CRRA_diff = abs(Pop_CRRA - Money_CRRA)) %>% 
  mutate(., Hybrid_CRRA_Alloc = Pop_CRRA * CRRA_diff) %>% 
  select(., c(Money_CRRA, Pop_CRRA, CRRA_diff,  Hybrid_CRRA_Alloc))
```

```{r}
esquisser(Survey_Dummy_Variables_Rowan_Simple_V2)
```

## X) Ordered probit models 

Maybe take a look at ordered probit models, they could be what you need:
https://en.wikipedia.org/wiki/Ordered_probit 

In statistics, ordered probit is a generalization of the widely used probit analysis to the case of more than two outcomes of an ordinal dependent variable (a dependent variable for which the potential values have a natural ordering, as in poor, fair, good, excellent). 
Ordered probit, like ordered logit, is a particular method of ordinal regression.

Another example application are Likert-type items commonly employed in survey research, where respondents rate their agreement on an ordered scale (e.g., "Strongly disagree" to "Strongly agree"). The ordered probit model provides an appropriate fit to these data, preserving the ordering of response options while making no assumptions of the interval distances between options.

The model cannot be consistently estimated using ordinary least squares; it is usually estimated using maximum likelihood. 

### a) Probit Money Risk attitude
```{r}
library("oglmx")

OP_Money_RiskAttitudes <- oprobit.reg(Money_CRRA ~ 
                          Age_grouped +
                          Geographic_group +
                          Duration_in_minutes_capped +
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                     #     Pop_CRRA +        # Probably too influential on this model
                          Alloc_diff_Round1to2 +
                          Alloc_diff_Round2to3 + 
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                           , data = as.data.frame(Survey_Dummy_Variables_Rowan_Simple_V1))
```

  
```{r}
summary(OP_Money_RiskAttitudes)
```

The Pr(>|z|) column shows the two-tailed p-values testing the null hypothesis that the 
coefficient is equal to zero (i.e. no significant effect). The usual value is 0.05, by this 
measure none of the coefficients have a significant effect on the log-odds ratio of the 
dependent variable. 

The z value also tests the null that the coefficient is equal to zero. For a 5% 
significance, the z-value should fall outside the ±1.96. 

The Estimate column shows the coefficients in log-odds form. What you get from this 
column is whether the effect of the predictors is positive or negative. 


# 4) Various plots and statistics

Let's start with simple stats about individual variables.
Then we will look at how they interact with each other.

Lists of stats I'd want to know / plot: 
- Average, min, max duration. Plot frequency 
- Sex ratio 
- Employed vs Student vs Retired. Numbers simply.
- University research % 
- EPA %
- IFI or other %
- Undergrad, postgrad 
- Conservation: Study / Work / Volunteering exp in %. 
  How many cumulated the three?
  How many cumulated work + study?
- Confidence in survey: I want a frequency/barplot.
- Char interest : I want a frequency/barplot.
- Papers : Who, how many ! 
- Sufficiently protected first and second: Compare means and variances.
  Maybe a plot with the two distributions.
- Anticipation of info: Just %. 
- Comfortable with experiment : Just % 
- Taxonomy : Just %. 
- TrueFalse Score : Average, min max. 

Things I'd like to test (in EXTRA MODELS) : 
- Do a model for duration to see if some were faster/slower. 
- Do a model for confidence in survey to understand who didn't trust it. 
- Do a model for char interest to see who love the char!
- Do a model for Anticipation of info to see what profiles anticipated. 
- Do a model for trueFalse Score to see who they are.

Also: 
- Show Pop and Money switches and CRRA into a table like in papers. 
- Find a way to plot the allocations in a nice way ! :) 

## I) Statistics 

### a) Duration 

Average, min, max duration. Plot frequency 
```{r}
summary(Survey_Dummy_Variables_Rowan_Simple_V1$Duration_in_minutes_capped)
```
Fuck one guy probably left the tab open and never closes her/his computer haha
```{r}
Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 ggplot() +
 aes(x = Duration_in_minutes_capped) +
 geom_histogram(bins = 30L, fill = "#2A4A82") +
 labs(x = "Time for completion of survey (mn)", y = "Count", title = "Distribution of completion times for the online survey", 
 subtitle = "Outlier values (i.e > 100mn) were excluded (Open tabs)") +
 theme_minimal()
```
### b) Sex ratio

```{r}
table(Survey_Dummy_Variables_Rowan_Simple_V1$Gender)

ggplot(Survey_Dummy_Variables_Rowan_Simple_V1) +
 aes(x = Geographic_group, fill = Gender) +
 geom_bar() +
 scale_fill_hue(direction = 1) +
 theme_minimal()
```
Percentages for each geographic group
```{r}
Gender_and_location <- Survey_Dummy_Variables_Rowan_V4 %>%  # We take V4 since it keeps the "Prefer not to say".
  select(., Gender, Geographic_group) %>% 
  filter(., !is.na(Gender)) %>% 
  filter(., !is.na(Geographic_group)) 

NumMales <- sum(Gender_and_location$Gender == "Male")
NumFemales <- sum(Gender_and_location$Gender == "Female")
NumNonBinary <- sum(Gender_and_location$Gender == "Non-binary")
NumGenderNeutral <- sum(Gender_and_location$Gender == "Gender neutral")
NumPrefNot2 <- sum(Gender_and_location$Gender == "Prefer not to say")

NumTotalGenders <- nrow(Gender_and_location)
```

Now do the maths
```{r}
paste(NumMales/NumTotalGenders *100,"% of males")
paste(NumFemales / NumTotalGenders *100,"% of females")
paste(NumNonBinary / NumTotalGenders *100, "% of non binary")
paste(NumGenderNeutral / NumTotalGenders *100, "% of gender-neutral")
paste(NumPrefNot2 / NumTotalGenders *100, "% of people who prefer not to say")
```


### c) Employed vs Student vs Retired. 

Numbers simply.
```{r}
Employement_df <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  select(., Employment_status, Geographic_group) %>% 
  filter(., !is.na(Employment_status)) %>% 
  filter(., !is.na(Geographic_group)) %>% 
  filter(., !Geographic_group == "WhereIsThat?") 

Total_nrow_Employement_df <- nrow(Employement_df)

paste((sum(with(Employement_df, Employment_status == "Employed")) / Total_nrow_Employement_df *100),"% of employed people")
paste((sum(with(Employement_df, Employment_status == "Unemployed")) / Total_nrow_Employement_df *100),"% of unemployed people")
paste((sum(with(Employement_df, Employment_status == "Student")) / Total_nrow_Employement_df *100),"% of students")
paste((sum(with(Employement_df, Employment_status == "Retired")) / Total_nrow_Employement_df *100),"% of retired people")
```
In North Atlantic Islands 
```{r}
North_Atlantic_Isles_nrow = sum(with(Employement_df, Geographic_group == "North_Atlantic_Isles"))

paste((sum(with(Employement_df, Employment_status == "Employed" & Geographic_group == "North_Atlantic_Isles")) / North_Atlantic_Isles_nrow *100),"% of employed people in the North_Atlantic_Isles")
paste((sum(with(Employement_df, Employment_status == "Unemployed" & Geographic_group == "North_Atlantic_Isles")) / North_Atlantic_Isles_nrow *100),"% of unemployed people in the North_Atlantic_Isles")
paste((sum(with(Employement_df, Employment_status == "Student" & Geographic_group == "North_Atlantic_Isles")) / North_Atlantic_Isles_nrow *100),"% of students in the North_Atlantic_Isles")
paste((sum(with(Employement_df, Employment_status == "Retired" & Geographic_group == "North_Atlantic_Isles")) / North_Atlantic_Isles_nrow *100),"% of retired people in the North_Atlantic_Isles")
```



### d) Position 

##### - General sample

```{r}
Position_df <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  select(., University_Research_Dummy, Environmental_Protection_Agency_Dummy, Wildlife_Conservation_Dummy, Other_position_Dummy, Geographic_group) %>% 
  filter(., !is.na(Geographic_group)) %>% 
  filter(., !Geographic_group == "WhereIsThat?") %>% 
  rowwise( ) %>% 
  mutate(., Multitaskers = sum(University_Research_Dummy, Environmental_Protection_Agency_Dummy, Wildlife_Conservation_Dummy, Other_position_Dummy)) 

Total_nrow_Position_df <- nrow(Position_df)
```

```{r}
paste((sum(with(Position_df, University_Research_Dummy == 1)) / Total_nrow_Position_df *100),"% of University people")
paste((sum(with(Position_df, Environmental_Protection_Agency_Dummy == 1)) / Total_nrow_Position_df *100),"% of EPA people")
paste((sum(with(Position_df, Wildlife_Conservation_Dummy == 1)) / Total_nrow_Position_df *100),"% of IFI-like people")
paste((sum(with(Position_df, Other_position_Dummy == 1)) / Total_nrow_Position_df *100),"% of other positions")
```

##### - North America sample

```{r}
Amerikka_nrow_Position = sum(with(Employement_df, Geographic_group == "North_America"))

paste((sum(with(Position_df, University_Research_Dummy == 1  & Geographic_group == "North_America")) / Amerikka_nrow_Position *100),"% of University people")
paste((sum(with(Position_df, Environmental_Protection_Agency_Dummy == 1  & Geographic_group == "North_America")) / Amerikka_nrow_Position *100),"% of EPA people")
paste((sum(with(Position_df, Wildlife_Conservation_Dummy == 1  & Geographic_group == "North_America")) / Amerikka_nrow_Position *100),"% of IFI-like people")
paste((sum(with(Position_df, Other_position_Dummy == 1  & Geographic_group == "North_America")) / Amerikka_nrow_Position *100),"% of other positions")
```

##### - Atlantic islands sample
```{r}
# Percentage of Public research people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, University_Research_Dummy==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) *100)

# Percentage of EPA people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Environmental_Protection_Agency_Dummy==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) *100)

# Percentage of IFI and co people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Wildlife_Conservation_Dummy==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) *100)
```

##### - Other European countries sample
```{r}
# Percentage of Public research people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, University_Research_Dummy==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) *100)

# Percentage of EPA people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Environmental_Protection_Agency_Dummy==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) *100)

# Percentage of IFI and co people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Wildlife_Conservation_Dummy==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) *100)

```
### e) Education 

##### - General sample
```{r}
# Percentage of Undergrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Undergrad_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) * 100)

# Percentage of Postgrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Postgrad_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)
```

##### - North America sample

```{r}
# Percentage of Undergrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Undergrad_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)

# Percentage of Postgrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Postgrad_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)
```

##### - Atlantic islands sample

```{r}
# Percentage of Undergrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Undergrad_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)

# Percentage of Postgrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Postgrad_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)
```

##### - Other European countries sample

```{r}
# Percentage of Undergrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Undergrad_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)

# Percentage of Postgrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Postgrad_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)
```
### f) Conservation experience 

##### - General sample
```{r}
# Percentage of Volunteering people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Volunteering_Exp_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) * 100)

# Percentage of people who studied in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)

# Percentage of of people who worked in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)
```

##### - North America sample

```{r}
# Percentage of Volunteering people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Volunteering_Exp_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)

# Percentage of of people who studied in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)

# Percentage of of people who worked in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Work_Exp_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)
```

##### - Atlantic islands sample

```{r}
# Percentage of Volunteering people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Volunteering_Exp_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)

# Percentage of of people who studied in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)

# Percentage of of people who worked in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Work_Exp_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)
```

##### - Other European countries sample

```{r}
# Percentage of Volunteering people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Volunteering_Exp_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)

# Percentage of of people who studied in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)

# Percentage of of people who worked in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Work_Exp_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)
```
  How many cumulated the three?
  
### g) Confidence in survey

```{r}
esquisser(Survey_Dummy_Variables_Rowan_Simple_V1)

ggplot(Survey_Dummy_Variables_Rowan_Simple_V1) +
 aes(x = Confidence_in_survey) +
 geom_density(adjust = 1L, 
 fill = "#112446") +
 labs(x = "Confidence of respondent in the survey", y = "Counts") +
 theme_minimal() +
 facet_wrap(vars(Publications_read))
```

The above is quite interesting. The plots are not in the right order but if you look at them in order you 
see that the more papers people have read about it, the more they agree with each other. 

Now let's see what average they have each: 

#### - General sample

```{r}
summary(Survey_Dummy_Variables_Rowan_Simple_V1$Confidence_in_survey)
```
##### - North America sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Geographic_group == "North_America"))
```

##### - Atlantic islands sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Geographic_group == "North_Atlantic_Isles"))
```

##### - Other European countries sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Geographic_group == "Other_Europe"))
```
##### - NO papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "0"))
```
##### - 1-5 papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "1-5"))
```

##### - 6-10 papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "6-10"))
```

##### - 11-20 papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "11-20"))
```

##### - 21+ papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "21+"))
```
### h) Papers : Who, how many ! 

```{r}
esquisser(Survey_Dummy_Variables_Rowan_Simple_V1)

library(ggplot2)

ggplot(Survey_Dummy_Variables_Rowan_Simple_V1) +
 aes(x = Publications_read) +
 geom_bar(fill = "#29467B") +
 labs(x = "Number of scientific publications regarding Arctic char read", y = "Counts") +
 theme_minimal() +
 scale_x_discrete(limits = c("0", "1-5", "6-10", "11-20", "21+"))
```
### i) Char interest 

```{r}
No_averaged_observations_char_interest <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  filter(., !between(Char_conservation_interest, 7.01, 7.99)) 
 
ggplot(No_averaged_observations_char_interest) +
 aes(x = Char_conservation_interest) +
 geom_histogram(bins = 7L, 
 fill = "#112446") +
 theme_minimal()
```
!!!! NA VALUES THAT WERE ARTIFICIALLY PUT ON THE AVERAGE WERE EXCLUDED HERE !!!! 

### j) Anticipation of new info
 
- Anticipation of info: %. 

##### - General sample
```{r}
Anticipation <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  filter(., !is.na(Information_anticipation))

# Percentage of people who anticipated:  
(sum(with(Anticipation, Information_anticipation == "Yes")) / nrow(Anticipation) * 100)

# Percentage of people who DID NOT anticipate:  
(sum(with(Anticipation, Information_anticipation == "No")) / nrow(Anticipation) * 100)
```

Loads of interesting plots ! 
Would require a model too I think!

!!! NA ARE EXCLUDED FROM THESE PLOTS !!! 

```{r} 
Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(!is.na(Information_anticipation)) %>%
 ggplot() +
 aes(x = Information_anticipation) +
 geom_bar(fill = "#112446") +
 theme_minimal()

Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(!is.na(Information_anticipation)) %>%
 ggplot() +
 aes(x = Age_grouped, fill = Information_anticipation) +
 geom_bar() +
 scale_fill_hue(direction = 1) +
 theme_minimal()

Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(!is.na(Information_anticipation)) %>%
 ggplot() +
 aes(x = Confidence_in_survey, fill = Information_anticipation) +
 geom_histogram(bins = 30L) +
 scale_fill_hue(direction = 1) +
 theme_minimal()

Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(!is.na(Information_anticipation)) %>%
 ggplot() +
 aes(x = Publications_read, fill = Information_anticipation) +
 geom_bar() +
 scale_fill_hue(direction = 1) +
 theme_minimal()

Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(!is.na(Information_anticipation)) %>%
 ggplot() +
 aes(x = TrueFalseScore, fill = Information_anticipation) +
 geom_histogram(bins = 30L) +
 scale_fill_hue(direction = 1) +
 theme_minimal()

Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(!is.na(Information_anticipation)) %>%
 ggplot() +
 aes(x = Alloc_profile_Round2to3, fill = Information_anticipation) +
 geom_bar() +
 scale_fill_hue(direction = 1) +
 theme_minimal()


Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(!is.na(Information_anticipation)) %>%
 ggplot() +
  aes(x = Alloc_diff_Round2to3) +
  geom_density(adjust = 1L, fill = "#526C9B") +
  theme_minimal() +
  facet_wrap(vars(Information_anticipation))

# A variant of the previous plot
ggplot(Anticipation, aes(x = Alloc_diff_Round2to3, fill = Information_anticipation)) + geom_density(alpha = 0.5) +
 labs(x = "Difference of budget allocation to High-Risk populations between Round two and three", y = "Density", title = "Niceplot") +
 theme_minimal()
```
The last three plots makes a lot of sense : 
The people who anticipated that "low risk" populations could be unique too have much less modifies there answers from Round two to three. 
Interestingly, some anticipated but did change it ... 

### k) Comfortable with experiment.

- Comfortable with experiment : Just % 

##### - General sample
```{r}
Comfortably <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  filter(., !is.na(Comfortable_with_Exp))

# Percentage of people who anticipated:  
(sum(with(Comfortably, Comfortable_with_Exp == "Yes")) / nrow(Comfortably) * 100)

# Percentage of people who DID NOT anticipate:  
(sum(with(Comfortably, Comfortable_with_Exp == "No")) / nrow(Comfortably) * 100)
```
Did that have something to see with the median duration ? 
```{r}
Comfortably %>%
  group_by(Comfortable_with_Exp) %>%
  summarise_at(vars(Duration_in_minutes), list(name = median))
```
### l) Taxonomy 

```{r}
Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(!is.na(Taxonomy)) %>%
 ggplot() +
 aes(x = Taxonomy) +
 geom_bar(fill = "#112446") +
 theme_minimal() +
 facet_wrap(vars(Publications_read))

# A variant of the previous plot
Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  filter(!is.na(Taxonomy)) %>%
  ggplot(., aes(x = Papers_read_fakeNumeric, fill = Taxonomy)) + geom_density(alpha = 0.7) +
 labs(x = "Number of papers read", y = "Density", title = "Niceplot") +
 theme_minimal()

Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(!is.na(Taxonomy)) %>%
 ggplot() +
 aes(x = Taxonomy) +
 geom_bar(fill = "#112446") +
 theme_minimal() +
 facet_wrap(vars(Many_Papers_Dummy)) +
 labs(x = "Before the survey, did you think that char taxonomy was inadequate ?", y = "Count", title = "Opinion on taxonomy depending on wether they have read a lot \n of papers about char (1) or not (0)") 
```


### m) TrueFalse Score 

Average, min max. 
```{r}

```


Plot 
```{r}
Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  filter(., !is.na(TrueFalseScore)) %>% 
  ggplot(., aes(x = TrueFalseScore, fill = as.factor(Papers_read_fakeNumeric))) + geom_density(alpha = 0.5) +
 labs(x = "True or False score", y = "Density", title = "True or false score varying with papers read") +
 theme_minimal()

Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  filter(., !is.na(TrueFalseScore)) %>% 
  ggplot(., aes(x = TrueFalseScore, fill = as.factor(Many_Papers_Dummy))) + geom_density(alpha = 0.5) +
 labs(x = "True or False score", y = "Density", title = "True or false score varying with papers read") +
 theme_minimal()

```


### n) Sufficiently protected

- Sufficiently protected first and second: Compare means and variances.
  Maybe a plot with the two distributions.
  
  The statement is "Arctic char populations in Ireland are sufficiently protected by general regulations on water quality so it is unnecessary to allocate more ressources to their conservation." 
```{r}
# Now a nice density plot
Sufficiently_protected_data_first <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  select(., Scale_Sufficently_Protected_First, name) %>% 
  rename("Scale_Sufficently_Protected" = Scale_Sufficently_Protected_First) %>% 
  mutate(., Time_asked = "First")
Sufficiently_protected_data_second <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  select(., Scale_Sufficently_Protected_Second, name) %>% 
  rename("Scale_Sufficently_Protected" = Scale_Sufficently_Protected_Second) %>% 
  mutate(., Time_asked = "Second")

Sufficiently_protected_data <- rbind(Sufficiently_protected_data_first, Sufficiently_protected_data_second)


Sufficiently_protected_data %>% 
ggplot(., aes(x = Scale_Sufficently_Protected, fill = Time_asked)) + 
  geom_density(alpha = 0.5) +
 labs(x = "Disagreement scale with suggestion that char are sufficiently protected in Ireland") +
 theme_minimal()
```

```{r}
t.test(Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_First, Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_Second, alternative = "less" )
```
Try it on different subsets.
```{r}
# Students
Susbet_Students <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
    filter(., Student_Dummy == 1)
t.test(Susbet_Students$Scale_Sufficently_Protected_First, Susbet_Students$Scale_Sufficently_Protected_Second, alternative = "less" )

# Char_conservation_interest
Susbet_Char_conservation_interest <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
    filter(., Char_conservation_interest < 7)
t.test(Susbet_Char_conservation_interest$Scale_Sufficently_Protected_First, Susbet_Char_conservation_interest$Scale_Sufficently_Protected_Second, alternative = "less" )
```



## II) Extra models 


Things I'd like to test (in EXTRA MODELS) : 
- Do a model for duration to see if some were faster/slower. 
- Do a model for confidence in survey to understand who didn't trust it. 
- Do a model for char interest to see who love the char!
- Do a model for Anticipation of info to see what profiles anticipated. 
- Do a model for trueFalse Score to see who they are.

### a) Duration model 

```{r}
Duration_model <- lm(formula = Duration_in_minutes_capped ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round2 +
                          Alloc_HR_Round3 +
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```

```{r}
alias( Duration_model )

car::vif(Duration_model)
```
  
  
```{r}
summary(Duration_model)
```
### b) Confidence in survey model 

```{r}
Confidence_model <- lm(formula = Confidence_in_survey ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Char_conservation_interest +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round2 +
                          Alloc_HR_Round3 +
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```

```{r}
alias( Confidence_model )

car::vif(Confidence_model)
```
  
  
```{r}
summary(Confidence_model)
```
Very interesting ! 
Old and medium-age people tend to have less confidence, 
while people who have read papers have more confidence ! 
The gretaest effect is for unemployed people, who trust the survey big time. 

Surprisingly, experience in conservation did not have an effect on that model ... ???


### c) Char interest model 

```{r}
CharInterest_model <- lm(formula = Char_conservation_interest ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey  +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round2 +
                          Alloc_HR_Round3 +
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```

```{r}
alias( CharInterest_model )

car::vif(CharInterest_model)
```
  
  
```{r}
summary(CharInterest_model)
```
That line is interesting potentially : 

Alloc_profile_AllRoundsSame-Up        -3.997681   1.638621  -2.440   0.0180 *  

That would mean that people who did not react to the information and then allocated more to the HR did not believe in char in the first place . 
Well it's not that interesting but we could work on that. 


### d) Anticipation model 

I NEED A LOGISTIC REGRESSION FOR THAT ! 

```{r}
Anticipation_model <- lm(formula = Information_anticipation  ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey  +
                          Publications_read +
                          Char_conservation_interest +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round2 +
                          Alloc_HR_Round3 +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```

```{r}
alias( Anticipation_model )

car::vif(Anticipation_model)
```
  
  
```{r}
summary(Anticipation_model)
```


### e) True or False model 

```{r}
TrueFalseScore_model <- lm(formula = TrueFalseScore  ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration_in_minutes_capped + 
                          Gender + 
                          Employment_status +
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey  +
                          Publications_read +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          Char_conservation_interest +
                          Money_CRRA +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round2 +
                          Alloc_HR_Round3 +
                          Information_anticipation +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```

```{r}
alias( TrueFalseScore_model )

car::vif(TrueFalseScore_model)
```

```{r}
summary(TrueFalseScore_model)
```
Ok, always good to confirm that people who have read many papers also had a better score at the true or false :) 
Our experts are not so bad haha 










# 5) Budget allocation figures 


```{r}
Risk_LR_Round1 = 0.2
Risk_HR_Round1 = 0.7

Budget_DF_Round1 <- data.frame(HR_Budg_Alloc =seq(0, 100, by=1), LR_Budg_Alloc = seq(100, 0, by=-1)) %>% 
  mutate(., HR_Pops_saved = HR_Budg_Alloc *0.01*10) %>% 
  mutate(., LR_Pops_saved = LR_Budg_Alloc *0.01*35) %>% 
  mutate(., Pop_saved_Total = HR_Pops_saved + LR_Pops_saved) %>% 
  mutate(., HR_Pops_Lost = ((10 - HR_Pops_saved) * Risk_HR_Round1)) %>% 
  mutate(., LR_Pops_Lost = ((35 - LR_Pops_saved) * Risk_LR_Round1)) %>% 
  mutate(., Pop_Lost_Total = HR_Pops_Lost + LR_Pops_Lost) %>% 
  mutate(., Percentage_HR_PopsLost = (HR_Pops_Lost / 10) *100) %>% 
  mutate(., Percentage_LR_PopsLost = (LR_Pops_Lost / 35) *100) 

# The same for round two and three
Risk_LR_Round2 = 0.4
Risk_HR_Round2 = 0.8

Budget_DF_Round2 <- data.frame(HR_Budg_Alloc =seq(0, 100, by=1), LR_Budg_Alloc = seq(100, 0, by=-1)) %>% 
  mutate(., HR_Pops_saved = HR_Budg_Alloc *0.01*10) %>% 
  mutate(., LR_Pops_saved = LR_Budg_Alloc *0.01*35) %>% 
  mutate(., Pop_saved_Total = HR_Pops_saved + LR_Pops_saved) %>% 
  mutate(., HR_Pops_Lost = ((10 - HR_Pops_saved) * Risk_HR_Round2)) %>% 
  mutate(., LR_Pops_Lost = ((35 - LR_Pops_saved) * Risk_LR_Round2)) %>% 
  mutate(., Pop_Lost_Total = HR_Pops_Lost + LR_Pops_Lost) %>% 
  mutate(., Percentage_HR_PopsLost = (HR_Pops_Lost / 10) *100) %>% 
  mutate(., Percentage_LR_PopsLost = (LR_Pops_Lost / 35) *100)  

esquisser(Budget_DF_Round2)
```


## Figure Round 1
```{r}
ggplot(Budget_DF_Round1) +
 aes(x = HR_Budg_Alloc, colour = Pop_Lost_Total) +
 geom_point(aes(y= Percentage_HR_PopsLost), shape = 17, size = 1.5, show.legend = T) +
 geom_point(aes(y= Percentage_LR_PopsLost), shape = 19, size = 1, show.legend = T) +
 labs(x = "Budget allocation to the HR category") +
  scale_y_continuous(
    # Features of the first axis
    name = "% of HR expected to go extinct (Triangles)",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*coeff, name="% of LR expected to go extinct (Circles)" )
  ) +
   scale_color_viridis_c(option = "viridis", direction = 1) +
 theme_minimal() + 
 theme(legend.position="right")
```


## Figure Round 2 & 3
```{r}
ggplot(Budget_DF_Round2) +
 aes(x = HR_Budg_Alloc, colour = Pop_Lost_Total) +
 geom_point(aes(y= Percentage_HR_PopsLost), shape = 17, size = 1.5, show.legend = T) +
 geom_point(aes(y= Percentage_LR_PopsLost), shape = 19, size = 1, show.legend = T) +
 labs(x = "Budget allocation to the HR category") +
  scale_y_continuous(
    # Features of the first axis
    name = "% of HR expected to go extinct (Triangles)",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~.*coeff, name="% of LR expected to go extinct (Circles)" )
  ) +
   scale_color_viridis_c(option = "viridis", direction = 1) +
 theme_minimal() 
```
















































































































































## END












This works
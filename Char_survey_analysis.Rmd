---
title: "Char_survey_analysis"
author: 'Noé Barthelemy'
date: "28/07/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Library
```{r}
library(tidyverse)
library(readxl)
library(esquisse)
library(nnet)
```

This is empty but I needed a script to try the sharing process with Rowan.

```{r}
Survey_Dummy_Variables_Rowan_V1_NB_edits <- read_excel("~/Bureau/PHD/Ongoing_Projects/Economics/DATA_SURVEY/Survey Dummy Variables_Rowan_V1_SEP2021.xlsx")

# Keep that one stored as backup object 
# Original_from_Rowan_BACKUP <- Survey_Dummy_Variables_Rowan_V1_NB_edits
```
Create some dummy variables to re-create what Rowan did originally by hand on excel: 
--> The current position is sorted.

```{r}
Survey_Dummy_Variables_Rowan_V1_September2021 <- Survey_Dummy_Variables_Rowan_V1_NB_edits %>% 
  mutate(., Male = ifelse(Gender == "Male", 1, 0)) %>%
  mutate(., Female = ifelse(Gender == "Female", 1, 0)) %>% 
  mutate(., Employed_Dummy = ifelse(Employment_status == "Employed", 1, 0)) %>% 
  mutate(., Retired_Dummy = ifelse(Employment_status == "Retired", 1, 0)) %>% 
  mutate(., Student_Dummy = ifelse(Employment_status == "Full time student / apprenticeship", 1, 0)) %>% 
  mutate(., Undergrad_Dummy = ifelse(Highest_education_level == "University - undergraduate degree", 1, 0)) %>% 
  mutate(., Postgrad_Dummy = ifelse(Highest_education_level == "University - postgraduate degree", 1, 0)) %>% 
  mutate(., No_Papers_Dummy = ifelse(Publications_read == "0", 1, 0)) %>% 
  mutate(., Some_Papers_Dummy = ifelse(Publications_read %in% c("1-5", "6-10", "11-20"), 1, 0)) %>% 
  mutate(., Many_Papers_Dummy = ifelse(Publications_read == "21+", 1, 0)) %>% 
  # Now sort the lottery thing: 
  mutate(., Q1_Risk_Rating = ifelse(Risk_lottery_money_99 == "Option A", 0, 1)) %>% 
  mutate(., Q2_Risk_Rating = ifelse(Risk_lottery_money_90 == "Option A", 0, 2)) %>% 
  mutate(., Q3_Risk_Rating = ifelse(Risk_lottery_money_80 == "Option A", 0, 3)) %>% 
  mutate(., Q4_Risk_Rating = ifelse(Risk_lottery_money_70 == "Option A", 0, 4)) %>% 
  mutate(., Q5_Risk_Rating = ifelse(Risk_lottery_money_60 == "Option A", 0, 5)) %>% 
  mutate(., Q6_Risk_Rating = ifelse(Risk_lottery_money_50 == "Option A", 0, 6)) %>% 
  mutate(., Q7_Risk_Rating = ifelse(Risk_lottery_money_40 == "Option A", 0, 7)) %>% 
  mutate(., Q8_Risk_Rating = ifelse(Risk_lottery_money_30 == "Option A", 0, 8)) %>% 
  mutate(., Q9_Risk_Rating = ifelse(Risk_lottery_money_20 == "Option A", 0, 9)) %>% 
  mutate(., Q10_Risk_Rating = ifelse(Risk_lottery_money_10 == "Option A", 0, 10)) %>% 
  rowwise(.) %>% 
  mutate(., Average_Lotto_Risk_Rating = mean(c(Q1_Risk_Rating, Q2_Risk_Rating, Q3_Risk_Rating, Q4_Risk_Rating, Q5_Risk_Rating, Q6_Risk_Rating, Q7_Risk_Rating, Q8_Risk_Rating, Q9_Risk_Rating, Q10_Risk_Rating))) %>% 
  # Now sort the population lottery thing: 
  mutate(., Q1_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_99 == "Option A", 0, 1)) %>% 
  mutate(., Q2_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_90 == "Option A", 0, 2)) %>% 
  mutate(., Q3_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_80 == "Option A", 0, 3)) %>% 
  mutate(., Q4_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_70 == "Option A", 0, 4)) %>% 
  mutate(., Q5_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_60 == "Option A", 0, 5)) %>% 
  mutate(., Q6_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_50 == "Option A", 0, 6)) %>% 
  mutate(., Q7_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_40 == "Option A", 0, 7)) %>% 
  mutate(., Q8_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_30 == "Option A", 0, 8)) %>% 
  mutate(., Q9_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_20 == "Option A", 0, 9)) %>% 
  mutate(., Q10_Conservation_Risk_Rating_ = ifelse(Risk_lottery_populations_10 == "Option A", 0, 10)) %>% 
  rowwise(.) %>% 
  mutate(., Average_Cons_Risk_Rating_ = mean(c(Q1_Conservation_Risk_Rating_, Q2_Conservation_Risk_Rating_, Q3_Conservation_Risk_Rating_, Q4_Conservation_Risk_Rating_, Q5_Conservation_Risk_Rating_, Q6_Conservation_Risk_Rating_, Q7_Conservation_Risk_Rating_, Q8_Conservation_Risk_Rating_, Q9_Conservation_Risk_Rating_, Q10_Conservation_Risk_Rating_)))

```

# Things to do 

Threshold 10% of dataset to make a variable significant in the regression. 

List of things to do : 
7) Manually (or with code) tidy the "Highest education level" variable. The "College" should be put as "Undergraduate", to reflect the higher education level that it is.

8) Discuss with George about the missing values and how to deal with them: Remove them, impute the mean observation, impute with a predictive model or create dummy variables to bypass the whole thing?  (Same thing for "Char conservation interest" and "Sufficiently protected")

For the true or false: ALSO, report in the text of the paper which answers were really common and interesting (eg. if people think that char are endangered).

9) For "Arctic char qualities", record the common answers and report it in the paper and discuss. 

11) Publications read: Keep the three dummies. Check correlation between that and the True or False score. 

Next meeting: 10/09/2021 

12) Get the CRRA into a code.

More info about the CRRA : http://library.lshtm.ac.uk/MSc_PHDC/2014-2015/108643.pdf 
*** 
What we did is replaced the "inf" value of CRRA by -2 for VERY risk lobing people
AND by -2.5 for the people who never switched ! 
***

After meeting : 

13) Check if the respondents who allocated more to HR in round 1 than in Round 2 (weird!) actually anticipated more than others that the LR pops could be unique too !

14) Find ways to optimise the model !










ABOUT NA'S in regression models: 
https://bookdown.org/egarpor/PM-UC3M/app-nas.html 




# 1) Dataset filtering and formatting. 

## A) Filter completion level and data missingness

For now we filter anything below 50 percent. 
```{r}
# At this point I'm also going to add random names to the answers using the babynames dataset
# Add a unique ID to each answer
 
library(babynames)
data(babynames)

Baby_names_only <- babynames %>% select(., name) %>% unique()

# Commented line below so that the names don't change once sampled! 
# Baby_names_sample <- sample_n(Baby_names_only, nrow(Survey_Dummy_Variables_Rowan_V1_September2021)) 
Survey_Dummy_Variables_Rowan_V1_September2021 <- cbind(Baby_names_sample, Survey_Dummy_Variables_Rowan_V1_September2021)

Survey_Dummy_Variables_Rowan_V2 <- Survey_Dummy_Variables_Rowan_V1_September2021 %>% 
  filter(., Progress >= 70) %>% 
  # Create a variable that summarises missing data.
  mutate(MissingData = rowSums(is.na(.))) %>% 
  filter(., MissingData <= 20) %>% 
  select(., MissingData, everything()) 
```
apply(mtcars, MARGIN = 1, function(x) sum(is.na(x)))

## B) Sort the "Country" variable.

First we clean it. 
```{r}
Survey_Dummy_Variables_Rowan_V3 <- Survey_Dummy_Variables_Rowan_V2 %>% 
  ## We need one row per special case.
  mutate(., Country = if_else(condition = Country == "uk", true = "UK", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "GB", true = "UK", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "United Kingdom", true = "UK", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "ireland", true = "Ireland", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "Ireland.", true = "Ireland", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "canada", true = "Canada", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "germany", true = "Germany", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "N.Ireland", true = "Northern_Ireland", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "Nlreland", true = "Northern_Ireland", false = Country)) %>% 
  mutate(., Country = if_else(condition = Country == "Northern Ireland", true = "Northern_Ireland", false = Country)) %>% 
  ## Special_case : No country entered. 
  mutate(., Country = if_else(condition = is.na(Country), true = "Unknown", false = Country)) %>% 
  select(., Country, everything())
```

Then we add variables that are going to summarise them in upper groups. 
```{r}
Survey_Dummy_Variables_Rowan_V4 <- Survey_Dummy_Variables_Rowan_V3 %>% 
  mutate(., Geographic_group = if_else(condition = Country %in% c("UK", "Ireland", "Northern_Ireland", "England", "Scotland", "Wales"), true = "North_Atlantic_Isles", false = if_else(condition = Country %in% c("Germany", "France", "Italy", "Norway", "Czech Republic", "Netherlands", "Sweden", "Denmark", "Finland", "Switzerland"), true = "Other_Europe", false = if_else(condition = Country %in% c("Canada", "USA"), true = "North_America", false = "WhereIsThat?" )))) %>% 
  select(., Country,Geographic_group, everything())
```

## C) Sort the "Year of Birth" variable.

```{r}
Survey_Dummy_Variables_Rowan_V5 <- Survey_Dummy_Variables_Rowan_V4 %>% 
  # Create the age variable
  mutate(., Age = 2021 - as.numeric(Year_of_birth)) %>% 
  # Group by class of age ! 
  mutate(., Age_grouped = if_else(Age <= 30, "EarlyAge", if_else(Age >= 31 & Age <= 60, "MediumAge", if_else(Age >= 61, "OldAge", "Problem!!")))) %>% 
  select(., Year_of_birth, Age,Age_grouped,  everything()) 
```

## D) Sort the "Current position" dummies.

```{r}
Survey_Dummy_Variables_Rowan_V6 <- Survey_Dummy_Variables_Rowan_V5 %>% 
  separate(., Current_position, into = c("Position1", "Position2", "Position3"), sep = ",") %>% 
  mutate(., University_Research_Dummy = if_else(condition = Position1 == "University (public) research" | Position2 == "University (public) research" | Position3 == "University (public) research", true = 1, false = 0, missing = 0)) %>%  
  mutate(., Environmental_Protection_Agency_Dummy = if_else(condition = Position1 == "Environmental protection agency" | Position2 == "Environmental protection agency" | Position3 == "Environmental protection agency", true = 1, false = 0, missing = 0)) %>%  
  mutate(., Wildlife_Conservation_Dummy = if_else(condition = Position1 == "Wildlife conservation/management agency" | Position2 == "Wildlife conservation/management agency" | Position3 == "Wildlife conservation/management agency", true = 1, false = 0, missing = 0)) %>% 
  # It did the job, let's filter out those useless columns: 
  select(., -c("Position1", "Position2","Position3"))
  
```

## E) Sort the conservation experience variable

  Change the conservation experience variable, put them all in dummy variables. 
  Use gsub to remov the "Yes,".
  
```{r}
Survey_Dummy_Variables_Rowan_V7 <- Survey_Dummy_Variables_Rowan_V6 %>% 
  mutate(., Volunteering_Exp_Dummy = ifelse(grepl(pattern = "volunteering", x = Conservation_experience) == T, yes = "1",  no =  "0")) %>% 
  mutate(., Work_Exp_Dummy = ifelse(grepl(pattern = "work", x = Conservation_experience) == T, yes = "1",  no =  "0")) %>% 
  mutate(., Study_Exp_Dummy = ifelse(grepl(pattern = "studies", x = Conservation_experience) == T, yes = "1",  no =  "0")) 
```


## F) Sort the "I don't know"'s.

Missing values and how to deal with them: Remove them, impute the mean observation, impute with a predictive model or create dummy variables to bypass the whole thing?  (Same thing for "Char conservation interest" and "Sufficiently protected")

Let's inspect how much missing data we have in the first place !
```{r}
Survey_Dummy_Variables_Rowan_V7 %>% 
  count(., Confidence_in_survey == "I don't know")
```
```{r}
Survey_Dummy_Variables_Rowan_V7 %>% 
  count(., Char_conservation_interest == "I don't know")
```
```{r}
Survey_Dummy_Variables_Rowan_V7 %>% 
  count(., Scale_Sufficently_Protected_First == "I don't know")
```
So quite low numbers after all. Let's turn them into the mean observation! 

Calculate the mean of observations:
```{r}
Mean_Confidence_in_survey = mean(as.numeric(Survey_Dummy_Variables_Rowan_V7$Confidence_in_survey), na.rm = T)
Mean_Char_conservation_interest = mean(as.numeric(Survey_Dummy_Variables_Rowan_V7$Char_conservation_interest), na.rm = T)
Mean_Scale_Sufficently_Protected_First = mean(as.numeric(Survey_Dummy_Variables_Rowan_V7$Scale_Sufficently_Protected_First), na.rm = T)
Mean_Scale_Sufficently_Protected_Second = mean(as.numeric(Survey_Dummy_Variables_Rowan_V7$Scale_Sufficently_Protected_Second), na.rm = T)

Mean_Confidence_in_survey
Mean_Char_conservation_interest
Mean_Scale_Sufficently_Protected_First
```
Change the "I don't know" for the mean values
```{r}
Survey_Dummy_Variables_Rowan_V8 <- Survey_Dummy_Variables_Rowan_V7 %>% 
  mutate(., Confidence_in_survey = ifelse(Confidence_in_survey == "I don't know", yes = Mean_Confidence_in_survey, no = Confidence_in_survey)) %>% 
  mutate(., Char_conservation_interest = ifelse(Char_conservation_interest == "I don't know", yes = Mean_Char_conservation_interest, no = Char_conservation_interest)) %>% 
  mutate(., Scale_Sufficently_Protected_First = ifelse(Scale_Sufficently_Protected_First == "I don't know", yes = Mean_Scale_Sufficently_Protected_First, no = Scale_Sufficently_Protected_First)) %>% 
  mutate(., Scale_Sufficently_Protected_Second = ifelse(Scale_Sufficently_Protected_Second == "I don't know", yes = Mean_Scale_Sufficently_Protected_Second, no = Scale_Sufficently_Protected_Second))
  
```

## G) Sort the "True or False"

For the "True or false" : Use regex to create a dummy variable for each answer, then compute a score: +1 point for each true answer, -1 for each negative. ALSO, report in the text of the paper which answers were really common and interesting (eg. if people think that char are endangered).

It is relatively easy but takes a few lines of code:
We'll create score variables for each value found, and then sum up this into a global score variable.
Then we remove the score variables and keep only the global score.
```{r}
# We need a function to operate the sum at the end
p_sum <- function(...) {
  rowSums(pick(...))
}


Survey_Dummy_Variables_Rowan_V9 <- Survey_Dummy_Variables_Rowan_V8 %>% 
  # The true ones first: 
  mutate(., TrueFalse_DV_3 = ifelse(grepl(pattern = "oldest", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_5 = ifelse(grepl(pattern = "phenotypic", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_6 = ifelse(grepl(pattern = "different", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_7 = ifelse(grepl(pattern = "extinct", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_9 = ifelse(grepl(pattern = "sensitive", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_12= ifelse(grepl(pattern = "colorful", x = True_false_question) == T, yes = 1,  no =  "0")) %>% 
  # Now the false one, minus one point: 
  mutate(., TrueFalse_DV_1 = ifelse(grepl(pattern = "farming", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_2 = ifelse(grepl(pattern = "IUCN", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_4 = ifelse(grepl(pattern = "Brown", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_8 = ifelse(grepl(pattern = "declined", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_10= ifelse(grepl(pattern = "Climate", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  mutate(., TrueFalse_DV_11= ifelse(grepl(pattern = "Anadromous", x = True_false_question) == T, yes = -1,  no =  "0")) %>% 
  # Make a score.
  mutate(., TrueFalseScore = as.numeric(TrueFalse_DV_1)+ as.numeric(TrueFalse_DV_2)+ as.numeric(TrueFalse_DV_3)+ as.numeric(TrueFalse_DV_4)+ as.numeric(TrueFalse_DV_5)+ as.numeric(TrueFalse_DV_6)+ as.numeric(TrueFalse_DV_7)+ as.numeric(TrueFalse_DV_8)+ as.numeric(TrueFalse_DV_9) +as.numeric(TrueFalse_DV_10)+ as.numeric(TrueFalse_DV_11) + as.numeric(TrueFalse_DV_12)) %>% 
  # Remove the crap.
  select(., -c(TrueFalse_DV_1, TrueFalse_DV_2, TrueFalse_DV_3, TrueFalse_DV_4, TrueFalse_DV_5, TrueFalse_DV_6, TrueFalse_DV_7,TrueFalse_DV_8,TrueFalse_DV_9,TrueFalse_DV_10,TrueFalse_DV_11,TrueFalse_DV_12))
```
## H) Calculate the "Lottery switch" and "CRRA" values

Basically, when are they switching from B to A ?

	CRRA
1	0.98
2	0.84
3	0.67
4	0.48
5	0.26
6	0
7	-0.33
8	-0.74
9	-1.33
10	inf  

Less than 0 is risk loving 	
0 is risk neutral 	
More than 0 is risk averse	

### I) For the money lottery
```{r}
Survey_Dummy_Variables_Rowan_V10 <- Survey_Dummy_Variables_Rowan_V9 %>% 
  mutate(., Money_Switch = ifelse(Risk_lottery_money_99 == "Option A", "99", ifelse(Risk_lottery_money_90 == "Option A", "90", ifelse(Risk_lottery_money_80 == "Option A", "80",ifelse(Risk_lottery_money_70 == "Option A", "70", ifelse(Risk_lottery_money_60 == "Option A", "60", ifelse(Risk_lottery_money_50 == "Option A", "50", ifelse(Risk_lottery_money_40 == "Option A", "40", ifelse(Risk_lottery_money_30 == "Option A", "30", ifelse(Risk_lottery_money_30 == "Option A", "30", ifelse(Risk_lottery_money_20 == "Option A", "20", ifelse(Risk_lottery_money_10 == "Option A", "10", "0"))) )))) ) )) )) %>% 
  
  # Get the CRRA into the code 
  mutate(., Money_CRRA = ifelse(Money_Switch == "99", 0.98, ifelse(Money_Switch == "90", 0.84, ifelse(Money_Switch == "80", 0.67, ifelse(Money_Switch == "70", 0.48, ifelse(Money_Switch == "60", 0.26, ifelse(Money_Switch == "50", 0, ifelse(Money_Switch == "40", -0.33, ifelse(Money_Switch == "30", -0.74, ifelse(Money_Switch == "20", -1.33, ifelse(Money_Switch == "10", -2, no = -2.5) )) ) )))))))
```

### II) For the population lottery
```{r}
Survey_Dummy_Variables_Rowan_V11 <- Survey_Dummy_Variables_Rowan_V10 %>% 
  mutate(., Pop_Switch = ifelse(Risk_lottery_populations_99 == "Option A", "99", ifelse(Risk_lottery_populations_90 == "Option A", "90", ifelse(Risk_lottery_populations_80 == "Option A", "80",ifelse(Risk_lottery_populations_70 == "Option A", "70", ifelse(Risk_lottery_populations_60 == "Option A", "60", ifelse(Risk_lottery_populations_50 == "Option A", "50", ifelse(Risk_lottery_populations_40 == "Option A", "40", ifelse(Risk_lottery_populations_30 == "Option A", "30", ifelse(Risk_lottery_populations_30 == "Option A", "30", ifelse(Risk_lottery_populations_20 == "Option A", "20", ifelse(Risk_lottery_populations_10 == "Option A", "10", "0"))) )))) ) )) )) %>% 
  
  # Get the CRRA into the code 
  mutate(., Pop_CRRA = ifelse(Pop_Switch == "99", 0.98, ifelse(Pop_Switch == "90", 0.84, ifelse(Pop_Switch == "80", 0.67, ifelse(Pop_Switch == "70", 0.48, ifelse(Pop_Switch == "60", 0.26, ifelse(Pop_Switch == "50", 0, ifelse(Pop_Switch == "40", -0.33, ifelse(Pop_Switch == "30", -0.74, ifelse(Pop_Switch == "20", -1.33, ifelse(Pop_Switch == "10", -2, no = -2.5) )) ) )))))))
```

## I) Sort the allocation budget variables

This is the new information given before Round 2: 

Initial round = All pops are of equal conservation status.
The 10 HR populations contains some unique genetic lineages.
Extinction of these = irremediable loss of genetic diversity of the oldest native fish species.

The interpretation is (From High Risk prospective): 
If from Round 1 to Round 2 allocation to HR goes UP : The respondent thought the HR were more valuable in the light of the new info (which highlights the uniqueness and the risk of huge loss). 
--> This is the most expected behavior.

If from Round 1 to Round 2 allocation to HR goes DOWN : Respondent thought the HR were not that valuable anymore. 
--> This is the less expected behavior. Maybe correlated to anticipation that LR can be unique too? 

If from Round 1 to Round 2 allocation to HR stays SAME : Respondent is unaffected by the info. 
--> Not expected. Either lack of understanding (test if correlated with duration) or maybe anticipation that LR can be unique too FROM THE START ! 
--> Unique case: If it stays the same at zero, could be that respondent attributes no value to the information compared to the fact that more populations can be saved by protecting the LR.

```{r}
Survey_Dummy_Variables_Rowan_V12 <- Survey_Dummy_Variables_Rowan_V11 %>% 
  
  # Create a nicer variable for Round 1:
  mutate(Alloc_LR_Round1 = ifelse(is.na(Risk_Reduction_LR_14_percent) & is.na(Risk_Reduction_LR_20_percent), NA, ifelse(is.na(Risk_Reduction_LR_20_percent), Risk_Reduction_LR_14_percent, ifelse(is.na(Risk_Reduction_LR_14_percent), Risk_Reduction_LR_20_percent, "Problem")))) %>% 
  mutate(Alloc_HR_Round1 = ifelse(is.na(Risk_Reduction_HR_50_percent) & is.na(Risk_Reduction_HR_70_percent), NA, ifelse(is.na(Risk_Reduction_HR_70_percent), Risk_Reduction_HR_50_percent, ifelse(is.na(Risk_Reduction_HR_50_percent), Risk_Reduction_HR_70_percent, "Problem")))) %>% 
  # Create a nicer variable for Round 2:
  mutate(Alloc_LR_Round2 = Risk_Reduction_LR_40_percent_Round_2) %>% 
  mutate(Alloc_HR_Round2 = Risk_Reduction_HR_80_percent_Round_2) %>% 
  # Create a nicer variable for Round 3:
  mutate(Alloc_LR_Round3 = Risk_Reduction_LR_40_percent_Round_3) %>% 
  mutate(Alloc_HR_Round3 = Risk_Reduction_HR_80_percent_Round_3) %>% 
  
  # Create a variable that summarises the allocation profile of a respondent: 
  # The idea is : Does the allocation goes up, or down? And then, up, down, the same?
  # To do that we need to put the focus on either the low risk or High risk.
  # I chose the High Risk allocation.
  mutate(., Alloc_profile_Round1to2 = ifelse(as.numeric(Alloc_HR_Round1) > as.numeric(Alloc_HR_Round2), "Down", ifelse(Alloc_HR_Round1 < Alloc_HR_Round2, "Up", ifelse(Alloc_HR_Round1 == Alloc_HR_Round2, "Same", "Problem")))) %>% 
  mutate(., Alloc_profile_Round2to3 = ifelse(Alloc_HR_Round2 > Alloc_HR_Round3, "Down", ifelse(Alloc_HR_Round2 < Alloc_HR_Round3, "Up", ifelse(Alloc_HR_Round2 == Alloc_HR_Round3, "Same", "Problem")))) %>% 

  unite("Alloc_profile_AllRounds", Alloc_profile_Round1to2, Alloc_profile_Round2to3, sep = "-", remove = F)  %>%   
  
  # Quantify this profile 
  mutate(., Alloc_diff_Round1to2 = as.numeric(Alloc_HR_Round2) - as.numeric(Alloc_HR_Round1)) %>% 
  mutate(., Alloc_diff_Round2to3 = as.numeric(Alloc_HR_Round3) - as.numeric(Alloc_HR_Round2))

```

```{r}
# Prepare some data for a plot : 
Allocation_diff_Round1to2 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(Alloc_diff_Round1to2) %>% 
  rename(., Alloc_diff = "Alloc_diff_Round1to2") %>% 
  mutate(., Round = "One to two")
Allocation_diff_Round2to3 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(Alloc_diff_Round2to3) %>% 
  rename(., Alloc_diff = "Alloc_diff_Round2to3") %>% 
  mutate(., Round = "Two to three")

Allocation_differences <- rbind(Allocation_diff_Round1to2, Allocation_diff_Round2to3)

# Now a nice density plot
ggplot(Allocation_differences, aes(x = Alloc_diff, fill = Round)) + geom_density(alpha = 0.5) +
 labs(x = "Difference of budget allocation to High-Risk populations between Rounds", y = "Density", title = "Niceplot") +
 theme_minimal()
```
```{r}
# Another plot to see the evolution in budget allocation 
Allocation_budgets1 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(., Alloc_HR_Round1, Alloc_LR_Round1, name) %>% 
  mutate(., Round = 1) %>% 
  rename(., Budget_alloc_HR = "Alloc_HR_Round1") %>% 
  rename(., Budget_alloc_LR = "Alloc_LR_Round1") 
Allocation_budgets2 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(., Alloc_HR_Round2, Alloc_LR_Round2, name)%>% 
  mutate(., Round = 2) %>% 
  rename(., Budget_alloc_HR = "Alloc_HR_Round2") %>% 
  rename(., Budget_alloc_LR = "Alloc_LR_Round2") 
Allocation_budgets3 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  select(., Alloc_HR_Round3, Alloc_LR_Round3, name)%>% 
  mutate(., Round = 3) %>% 
  rename(., Budget_alloc_HR = "Alloc_HR_Round3") %>% 
  rename(., Budget_alloc_LR = "Alloc_LR_Round3") 


Allocation_budgets <- rbind(Allocation_budgets1, Allocation_budgets2, Allocation_budgets3)
Allocation_budgets$Budget_alloc_HR <- as.numeric(Allocation_budgets$Budget_alloc_HR)
Allocation_budgets$Budget_alloc_LR <- as.numeric(Allocation_budgets$Budget_alloc_LR)


```
# NEED HELP TO PLOT THAT

```{r}
mean(Survey_Dummy_Variables_Rowan_V12$Alloc_diff_Round1to2, na.rm = T)
mean(Survey_Dummy_Variables_Rowan_V12$Alloc_diff_Round2to3, na.rm = T)
mean(as.numeric(Allocation_budgets1$Budget_alloc_HR), na.rm = T)
mean(as.numeric(Allocation_budgets1$Budget_alloc_LR), na.rm = T)
mean(Allocation_budgets2$Budget_alloc_HR, na.rm = T)
mean(Allocation_budgets2$Budget_alloc_LR, na.rm = T)
mean(Allocation_budgets3$Budget_alloc_HR, na.rm = T)
mean(Allocation_budgets3$Budget_alloc_LR, na.rm = T)
```
## J) Sort the "did you feel comfortable" variable 

When the people didn't check Yes or No and commented I assumed that it was a "YES".

```{r}
Survey_Dummy_Variables_Rowan_V13 <- Survey_Dummy_Variables_Rowan_V12 %>% 
  rename(., "Comfortable_with_Exp" = `Did_you_feel_comfortable_with_the_budget_allocation_experiment?_-_Selected_Choice`) %>% 
  mutate(., Comfortable_with_Exp = ifelse(Comfortable_with_Exp == "No", "No", ifelse(Comfortable_with_Exp == "Yes", "Yes", ifelse(Comfortable_with_Exp == "No,Would you like to comment it so we can improve it ?", "No", ifelse(Comfortable_with_Exp == "Yes,Would you like to comment it so we can improve it ?", "Yes",  ifelse(is.na(Comfortable_with_Exp), NA, "Yes")))) )) 
```

## K) Sort the Duration variable

```{r}
Survey_Dummy_Variables_Rowan_V14 <- Survey_Dummy_Variables_Rowan_V13 %>% 
  mutate(., Duration_in_minutes = Duration/60)
```

## L) Simple version 

Make a Simple version of the table
```{r}
Survey_Dummy_Variables_Rowan_Simple_V1 <- Survey_Dummy_Variables_Rowan_V14 %>% 
  select(., - c(Year_of_birth, Age, Employment_status, Highest_education_level, Conservation_experience, Useless, True_false_question, Risk_lottery_money_99, Risk_lottery_money_90, Risk_lottery_money_80, Risk_lottery_money_70, Risk_lottery_money_60, Risk_lottery_money_50, Risk_lottery_money_40, Risk_lottery_money_30, Risk_lottery_money_20, Risk_lottery_money_10, Risk_lottery_populations_99, Risk_lottery_populations_90, Risk_lottery_populations_80, Risk_lottery_populations_70, Risk_lottery_populations_60, Risk_lottery_populations_50, Risk_lottery_populations_40, Risk_lottery_populations_30, Risk_lottery_populations_20, Risk_lottery_populations_10, Q1_Risk_Rating, Q2_Risk_Rating, Q3_Risk_Rating, Q4_Risk_Rating, Q5_Risk_Rating, Q6_Risk_Rating, Q7_Risk_Rating, Q8_Risk_Rating, Q9_Risk_Rating, Q10_Risk_Rating, Q1_Conservation_Risk_Rating_, Q2_Conservation_Risk_Rating_, Q3_Conservation_Risk_Rating_, Q4_Conservation_Risk_Rating_, Q5_Conservation_Risk_Rating_, Q6_Conservation_Risk_Rating_, Q7_Conservation_Risk_Rating_, Q8_Conservation_Risk_Rating_, Q9_Conservation_Risk_Rating_, Q10_Conservation_Risk_Rating_, Risk_Reduction_LR_14_percent, Risk_Reduction_LR_20_percent, Risk_Reduction_HR_70_percent, Risk_Reduction_HR_50_percent, Risk_Reduction_LR_40_percent_Round_2, Risk_Reduction_HR_80_percent_Round_2, Risk_Reduction_LR_40_percent_Round_3, Risk_Reduction_HR_80_percent_Round_3))  %>% 
  
  # FILTER OUT MORE NA'S TO SOLVE THE MISSING DATA PROBLEM
filter(., !is.na(Alloc_diff_Round2to3))
  

Survey_Dummy_Variables_Rowan_Simple_V1$Work_Exp_Dummy <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Work_Exp_Dummy)
Survey_Dummy_Variables_Rowan_Simple_V1$Study_Exp_Dummy <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Study_Exp_Dummy)
Survey_Dummy_Variables_Rowan_Simple_V1$Volunteering_Exp_Dummy <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Volunteering_Exp_Dummy)
Survey_Dummy_Variables_Rowan_Simple_V1$Confidence_in_survey <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Confidence_in_survey)
Survey_Dummy_Variables_Rowan_Simple_V1$Char_conservation_interest <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Char_conservation_interest)
Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_First <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_First)
Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_Second <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_Second)
Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_HR_Round1 <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_HR_Round1)
Survey_Dummy_Variables_Rowan_Simple_V1$Money_Switch <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Money_Switch)
Survey_Dummy_Variables_Rowan_Simple_V1$Pop_Switch <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Pop_Switch)
Survey_Dummy_Variables_Rowan_Simple_V1$Pop_CRRA <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Pop_CRRA)
Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_diff_Round2to3 <- as.numeric(Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_diff_Round2to3)
```

```{r}
# summary(Survey_Dummy_Variables_Rowan_Simple_V1)
```

# 2) Some plotting: 

```{r}
# esquisser(Survey_Dummy_Variables_Rowan_Simple_V1)
```


# 3) Now the model


## Preamble - How to interpret a model

1) Look at the Residuals

The next item in the model output talks about the residuals. Residuals are essentially the difference between the actual observed response values (distance to stop dist in our case) and the response values that the model predicted. The Residuals section of the model output breaks it down into 5 summary points. When assessing how well the model fit the data, you should look for a symmetrical distribution across these points on the mean value zero (0). In our example, we can see that the distribution of the residuals do not appear to be strongly symmetrical. That means that the model predicts certain points that fall far away from the actual observed points. We could take this further consider plotting the residuals to see whether this normally distributed, etc. but will skip this for this example.

2) Coefficients

The next section in the model output talks about the coefficients of the model. Theoretically, in simple linear regression, the coefficients are two unknown constants that represent the intercept and slope terms in the linear model. If we wanted to predict the Distance required for a car to stop given its speed, we would get a training set and produce estimates of the coefficients to then use it in the model formula. Ultimately, the analyst wants to find an intercept and a slope such that the resulting fitted line is as close as possible to the 50 data points in our data set.

Coefficient - Estimate

The coefficient Estimate contains two rows; the first one is the intercept. The intercept, in our example, is essentially the expected value of the distance required for a car to stop when we consider the average speed of all cars in the dataset. In other words, it takes an average car in our dataset 42.98 feet to come to a stop. The second row in the Coefficients is the slope, or in our example, the effect speed has in distance required for a car to stop. The slope term in our model is saying that for every 1 mph increase in the speed of a car, the required distance to stop goes up by 3.9324088 feet.

Coefficient - Standard Error

The coefficient Standard Error measures the average amount that the coefficient estimates vary from the actual average value of our response variable. We’d ideally want a lower number relative to its coefficients. In our example, we’ve previously determined that for every 1 mph increase in the speed of a car, the required distance to stop goes up by 3.9324088 feet. The Standard Error can be used to compute an estimate of the expected difference in case we ran the model again and again. In other words, we can say that the required distance for a car to stop can vary by 0.4155128 feet. The Standard Errors can also be used to compute confidence intervals and to statistically test the hypothesis of the existence of a relationship between speed and distance required to stop.

Coefficient - t value

The coefficient t-value is a measure of how many standard deviations our coefficient estimate is far away from 0. We want it to be far away from zero as this would indicate we could reject the null hypothesis - that is, we could declare a relationship between speed and distance exist. In our example, the t-statistic values are relatively far away from zero and are large relative to the standard error, which could indicate a relationship exists. In general, t-values are also used to compute p-values.

Coefficient - Pr(>t)

The Pr(>t) acronym found in the model output relates to the probability of observing any value equal or larger than t. A small p-value indicates that it is unlikely we will observe a relationship between the predictor (speed) and response (dist) variables due to chance. Typically, a p-value of 5% or less is a good cut-off point. In our model example, the p-values are very close to zero. Note the ‘signif. Codes’ associated to each estimate. Three stars (or asterisks) represent a highly significant p-value. Consequently, a small p-value for the intercept and the slope indicates that we can reject the null hypothesis which allows us to conclude that there is a relationship between speed and distance.

Residual Standard Error

Residual Standard Error is measure of the quality of a linear regression fit. Theoretically, every linear model is assumed to contain an error term E. Due to the presence of this error term, we are not capable of perfectly predicting our response variable (dist) from the predictor (speed) one. The Residual Standard Error is the average amount that the response (dist) will deviate from the true regression line. In our example, the actual distance required to stop can deviate from the true regression line by approximately 15.3795867 feet, on average. In other words, given that the mean distance for all cars to stop is 42.98 and that the Residual Standard Error is 15.3795867, we can say that the percentage error is (any prediction would still be off by) 35.78%. It’s also worth noting that the Residual Standard Error was calculated with 48 degrees of freedom. Simplistically, degrees of freedom are the number of data points that went into the estimation of the parameters used after taking into account these parameters (restriction). In our case, we had 50 data points and two parameters (intercept and slope).

Multiple R-squared, Adjusted R-squared

The R-squared (R2
) statistic provides a measure of how well the model is fitting the actual data. It takes the form of a proportion of variance. R2 is a measure of the linear relationship between our predictor variable (speed) and our response / target variable (dist). It always lies between 0 and 1 (i.e.: a number near 0 represents a regression that does not explain the variance in the response variable well and a number close to 1 does explain the observed variance in the response variable). In our example, the R2 we get is 0.6510794. Or roughly 65% of the variance found in the response variable (dist) can be explained by the predictor variable (speed). Step back and think: If you were able to choose any metric to predict distance required for a car to stop, would speed be one and would it be an important one that could help explain how distance would vary based on speed? I guess it’s easy to see that the answer would almost certainly be a yes. That why we get a relatively strong R2. Nevertheless, it’s hard to define what level of R2

is appropriate to claim the model fits well. Essentially, it will vary with the application and the domain studied.

A side note: In multiple regression settings, the R2
will always increase as more variables are included in the model. That’s why the adjusted R2

is the preferred measure as it adjusts for the number of variables considered.

F-Statistic

F-statistic is a good indicator of whether there is a relationship between our predictor and the response variables. The further the F-statistic is from 1 the better it is. However, how much larger the F-statistic needs to be depends on both the number of data points and the number of predictors. Generally, when the number of data points is large, an F-statistic that is only a little bit larger than 1 is already sufficient to reject the null hypothesis (H0 : There is no relationship between speed and distance). The reverse is true as if the number of data points is small, a large F-statistic is required to be able to ascertain that there may be a relationship between predictor and response variables. In our example the F-statistic is 89.5671065 which is relatively larger than 1 given the size of our data.




In the regression we include : 

Gender, Birth, Country, Employment (Use employment VS study, use the dummy variables), Current position (University_DV + EPA_DV + Wildlife_DV), Highest education (Use only the dummies "Undergradute" and "Postgraduate"), Conservation experience (Use all the three as dummy variables), Confidence in Survey (once "I don't know" values are sorted), Char conservation interest, True or false score, Publications read (Use the three dummies), . 


## I) Model Round 1) 

These budget allocation models will use the allocation amount to HR populations as a response variable, 
and all the remaining variables as explanatory.

We will try this Round per Round first and see what we get.

+ = I took this one into the model

 [1] "Age_grouped"                            +                                                     
 [2] "Country"                                                                          
 [3] "Geographic_group"                       +                                                    
 [4] "name"                                                                             
 [5] "Progress"                                                                         
 [6] "Duration"                               +                                                    
 [7] "Finished"                                                                         
 [8] "Gender"                                 +                                                    
 [9] "Male"                                                                                       
[10] "Female"                                                                           
[11] "Employed_Dummy"                         +                                                    
[12] "Student_Dummy"                          +                                                    
[13] "Retired_Dummy"                          +                                                    
[14] "University_Research_Dummy"              +                                          
[15] "Environmental_Protection_Agency_Dummy"  +                                          
[16] "Wildlife_Conservation_Dummy"            +                                          
[17] "Current_position_other"                                                           
[18] "Undergrad_Dummy"                        +                                          
[19] "Postgrad_Dummy"                         +                                          
[20] "Highest_education_level_other"                                                    
[21] "Work_Exp_Dummy"                         +                                          
[22] "Study_Exp_Dummy"                        +                                          
[23] "Volunteering_Exp_Dummy"                 +                                          
[24] "Confidence_in_survey"                   +                                          
[25] "Char_conservation_interest"             +                                          
[26] "Arctic_char_qualities"                                                            
[27] "No_Papers_Dummy"                        +                                          
[28] "Some_Papers_Dummy"                      +                                          
[29] "Many_Papers_Dummy"                      +                                          
[30] "Scale_Sufficently_Protected_First"      +                                          
[31] "Average_Lotto_Risk_Rating"                                                        
[32] "Average_Cons_Risk_Rating_"                                                        
[33] "Information_anticipation"                                                         
[34] "Did_you_feel_comfortable_with_
the_budget_allocation_experiment?_-_Selected_Choice"
[35] "Comfortable_with_experiment"                                                      
[36] "Scale_Sufficently_Protected_Second"     +                                          
[37] "Taxonomy"                                                                         
[38] "Survey_comments"                                                                  
[39] "Group"                                  +                                          
[40] "TrueFalseScore"                         +                                          
[41] "Money_Switch"                           +                                          
[42] "Money_CRRA"                             +                                          
[43] "Pop_Switch"                             +                                          
[44] "Pop_CRRA"                               +                                          
[45] "Alloc_LR_Round1"                                                                  
[46] "Alloc_HR_Round1"                                                                  
[47] "Alloc_LR_Round2"                                                                  
[48] "Alloc_HR_Round2"                        +                                          
[49] "Alloc_LR_Round3"                                                                  
[50] "Alloc_HR_Round3"                        +                                          
[51] "Alloc_profile_AllRounds"                +                                          
[52] "Alloc_profile_Round1to2"                                                          
[53] "Alloc_profile_Round2to3"                                                          
[54] "Alloc_diff_Round1to2"                                                             
[55] "Alloc_diff_Round2to3"   

```{r}
Linearmod_test_Round1 <- lm(formula = Alloc_HR_Round1 ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration + 
                          Gender + 
                          Employed_Dummy + 
                          Student_Dummy + 
                          Retired_Dummy + 
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          # No_Papers_Dummy +  Removed as in total collinearity with Many papers dummy
                          Some_Papers_Dummy +
                          Many_Papers_Dummy +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_Switch +
                          Money_CRRA +
                          Pop_Switch +
                          Pop_CRRA + 
                          Alloc_HR_Round2 +
                          Alloc_HR_Round3 +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```
  
```{r}
summary(Linearmod_test_Round1)
```
   
## II) Model Round 2)

```{r}
Linearmod_test_Round2 <- lm(formula = Alloc_HR_Round2 ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration + 
                          Gender + 
                          Employed_Dummy + 
                          Student_Dummy + 
                          Retired_Dummy + 
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          # No_Papers_Dummy +  Removed as in total collinearity with Many papers dummy
                          Some_Papers_Dummy +
                          Many_Papers_Dummy +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_Switch +
                          Money_CRRA +
                          Pop_Switch +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round3 +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```
  
```{r}
summary(Linearmod_test_Round2)
```

## III) Model Round 3) 

```{r}
Linearmod_test_Round3 <- lm(formula = Alloc_HR_Round3 ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration + 
                          Gender + 
                          Employed_Dummy + 
                          Student_Dummy + 
                          Retired_Dummy + 
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          # No_Papers_Dummy +  Removed as in total collinearity with Many papers dummy
                          Some_Papers_Dummy +
                          Many_Papers_Dummy +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_Switch +
                          Money_CRRA +
                          Pop_Switch +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round2 +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```
  
```{r}
summary(Linearmod_test_Round3)
```

## IV) Multinomial logistic reg: Model on profile

Based on profile like Up-Down, Down-Down etc.
Since the response variable is categorical we need a multinomial logistic regression.

https://datasciencebeginners.com/2018/12/20/multinomial-logistic-regression-using-r/

```{r}
# Setting the basline 
Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_profile_AllRounds <- relevel(as.factor(Survey_Dummy_Variables_Rowan_Simple_V1$Alloc_profile_AllRounds), ref = "Same-Same")

Linearmod_test_Profile <- multinom(formula = Alloc_profile_AllRounds ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration + 
                          Gender + 
                          Employed_Dummy + 
                          Student_Dummy + 
                          Retired_Dummy + 
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          # No_Papers_Dummy +  Removed as in total collinearity with Many papers dummy
                          Some_Papers_Dummy +
                          Many_Papers_Dummy +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_Switch +
                          Money_CRRA +
                          Pop_Switch +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_HR_Round2 +
                          Alloc_HR_Round3 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```

# NO CLUE OF HOW TO INTERPRET THIS
```{r}
summary(Linearmod_test_Profile)
```
## V) Model Diff Round 1 to 2 

Now the goal is to make a model that predicts the difference of allocation between Round 1 and Round 2. 
This should be interesting. 
Then we will do the same with Round 2 to 3. 
I think that a linear regression will do the job! 

```{r}
Linearmod_Diff_Round1to2 <- lm(formula = Alloc_diff_Round1to2 ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration + 
                          Gender + 
                          Employed_Dummy + 
                          Student_Dummy + 
                          Retired_Dummy + 
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          # No_Papers_Dummy +  Removed as in total collinearity with Many papers dummy
                          Some_Papers_Dummy +
                          Many_Papers_Dummy +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_Switch +
                          Money_CRRA +
                          Pop_Switch +
                          Pop_CRRA + 
                          Alloc_HR_Round3 +
                          Alloc_profile_AllRounds 
                        
                        , data = Survey_Dummy_Variables_Rowan_Simple_V1) 
```
  
```{r}
summary(Linearmod_Diff_Round1to2)
```

## VI) Model Diff Round 2 to 3

We do the same with Round 2 to 3. 

```{r}
Linearmod_Diff_Round2to3 <- lm(formula = Alloc_diff_Round2to3 ~ 
                          Age_grouped + 
                          Geographic_group + 
                          Duration + 
                          Gender + 
                          Employed_Dummy + 
                          Student_Dummy + 
                          Retired_Dummy + 
                          University_Research_Dummy +
                          Environmental_Protection_Agency_Dummy +
                          Wildlife_Conservation_Dummy +
                          Undergrad_Dummy + 
                          Postgrad_Dummy +
                          Work_Exp_Dummy +
                          Study_Exp_Dummy +
                          Volunteering_Exp_Dummy +
                          Confidence_in_survey +
                          Char_conservation_interest +
                          # No_Papers_Dummy +  Removed as in total collinearity with Many papers dummy
                          Some_Papers_Dummy +
                          Many_Papers_Dummy +
                          Scale_Sufficently_Protected_First +
                          Scale_Sufficently_Protected_Second +
                          Group +
                          TrueFalseScore +
                          Money_Switch +
                          Money_CRRA +
                          Pop_Switch +
                          Pop_CRRA + 
                          Alloc_HR_Round1 +
                          Alloc_profile_AllRounds 
                        
                        , data = as.data.frame(Survey_Dummy_Variables_Rowan_Simple_V1)) 
```
  
```{r}
summary(Linearmod_Diff_Round2to3)
```

# 4) Various plots and statistics

Let's start with simple stats about individual variables.
Then we will look at how they interact with each other.

Lists of stats I'd want to know / plot: 
- Average, min, max duration. Plot frequency 
- Sex ratio 
- Employed vs Student vs Retired. Numbers simply.
- University research % 
- EPA %
- IFI or other %
- Undergrad, postgrad 
- Conservation: Study / Work / Volunteering exp in %. 
  How many cumulated the three?
  How many cumulated work + study?
- Confidence in survey: I want a frequency/barplot.
- Char interest : I want a frequency/barplot.
- Papers : Who, how many ! 
- Sufficiently protected first and second: Compare means and variances.
  Maybe a plot with the two distributions.
- Anticipation of info: Just %. 
- Comfortable with experiment : Just % 
- Taxonomy : Just %. 
- TrueFalse Score : Average, min max. 

Things I'd like to test : 
- Do a model for duration to see if some were faster/slower. 
- Do a model for confidence in survey to understand who didn't trust it. 
- Do a model for char interest to see who love the char!
- Do a model for Anticipation of info to see what profiles anticipated. 
- Do a model for trueFalse Score to see who they are. 

Also: 
- Show Pop and Money switches and CRRA into a table like in papers. 
- Find a way to plot the allocations in a nice way ! :) 

## I) Statistics 

### a) Duration 

Average, min, max duration. Plot frequency 
```{r}
summary(Survey_Dummy_Variables_Rowan_Simple_V1$Duration)

Duration_in_minutes <- Survey_Dummy_Variables_Rowan_Simple_V1$Duration/60
summary(Duration_in_minutes)
```
Fuck one guy probably left the tab open and never closes her/his computer haha

```{r}
library(dplyr)
library(ggplot2)


Survey_Dummy_Variables_Rowan_Simple_V1 %>%
 filter(Duration_in_minutes >= 4 & Duration_in_minutes <= 100) %>%
 ggplot() +
 aes(x = Duration_in_minutes) +
 geom_histogram(bins = 30L, fill = "#2A4A82") +
 labs(x = "Time for completion of survey (mn)", y = "Count", title = "Distribution of completion times for the online survey", 
 subtitle = "Outlier values (i.e > 100mn) were excluded (Open tabs)") +
 theme_minimal()
```
### b) Sex ratio

```{r}
table(Survey_Dummy_Variables_Rowan_Simple_V1$Gender)

library(ggplot2)

ggplot(Survey_Dummy_Variables_Rowan_Simple_V1) +
 aes(x = Geographic_group, fill = Gender) +
 geom_bar() +
 scale_fill_hue(direction = 1) +
 theme_minimal()
```
### c) Employed vs Student vs Retired. 

Numbers simply.
```{r}
# Percentage of Employed people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Employed_Dummy==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)

# Percentage of Student people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Student_Dummy==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)

# Percentage of Retired people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Retired_Dummy==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)
```
### d) Position 

##### - General sample
```{r}
# Percentage of Public research people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, University_Research_Dummy==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)

# Percentage of EPA people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Environmental_Protection_Agency_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)

# Percentage of IFI and co people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Wildlife_Conservation_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)
```
##### - North America sample
```{r}
# Percentage of Public research people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, University_Research_Dummy==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) *100)

# Percentage of EPA people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Environmental_Protection_Agency_Dummy==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) *100)

# Percentage of IFI and co people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Wildlife_Conservation_Dummy==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) *100)

```

##### - Atlantic islands sample
```{r}
# Percentage of Public research people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, University_Research_Dummy==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) *100)

# Percentage of EPA people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Environmental_Protection_Agency_Dummy==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) *100)

# Percentage of IFI and co people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Wildlife_Conservation_Dummy==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) *100)
```

##### - Other European countries sample
```{r}
# Percentage of Public research people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, University_Research_Dummy==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) *100)

# Percentage of EPA people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Environmental_Protection_Agency_Dummy==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) *100)

# Percentage of IFI and co people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Wildlife_Conservation_Dummy==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) *100)

```
### e) Education 

##### - General sample
```{r}
# Percentage of Undergrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Undergrad_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) * 100)

# Percentage of Postgrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Postgrad_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)
```

##### - North America sample

```{r}
# Percentage of Undergrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Undergrad_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)

# Percentage of Postgrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Postgrad_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)
```

##### - Atlantic islands sample

```{r}
# Percentage of Undergrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Undergrad_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)

# Percentage of Postgrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Postgrad_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)
```

##### - Other European countries sample

```{r}
# Percentage of Undergrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Undergrad_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)

# Percentage of Postgrad people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Postgrad_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)
```
### f) Conservation experience 

##### - General sample
```{r}
# Percentage of Volunteering people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Volunteering_Exp_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) * 100)

# Percentage of people who studied in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)

# Percentage of of people who worked in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1)) / nrow(Survey_Dummy_Variables_Rowan_Simple_V1) *100)
```

##### - North America sample

```{r}
# Percentage of Volunteering people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Volunteering_Exp_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)

# Percentage of of people who studied in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)

# Percentage of of people who worked in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Work_Exp_Dummy ==1 & Geographic_group == "North_America")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_America")) * 100)
```

##### - Atlantic islands sample

```{r}
# Percentage of Volunteering people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Volunteering_Exp_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)

# Percentage of of people who studied in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)

# Percentage of of people who worked in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Work_Exp_Dummy ==1 & Geographic_group == "North_Atlantic_Isles")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "North_Atlantic_Isles")) * 100)
```

##### - Other European countries sample

```{r}
# Percentage of Volunteering people: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Volunteering_Exp_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)

# Percentage of of people who studied in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Study_Exp_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)

# Percentage of of people who worked in conservation: 
(sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Work_Exp_Dummy ==1 & Geographic_group == "Other_Europe")) / sum(with(Survey_Dummy_Variables_Rowan_Simple_V1, Geographic_group == "Other_Europe")) * 100)
```
  How many cumulated the three?
  
### g) Confidence in survey

```{r}
esquisser(Survey_Dummy_Variables_Rowan_Simple_V1)

ggplot(Survey_Dummy_Variables_Rowan_Simple_V1) +
 aes(x = Confidence_in_survey) +
 geom_density(adjust = 1L, 
 fill = "#112446") +
 labs(x = "Confidence of respondent in the survey", y = "Counts") +
 theme_minimal() +
 facet_wrap(vars(Publications_read))
```

The above is quite interesting. The plots are not in the right order but if you look at them in order you 
see that the more papers people have read about it, the more they agree with each other. 

Now let's see what average they have each: 

#### - General sample

```{r}
summary(Survey_Dummy_Variables_Rowan_Simple_V1$Confidence_in_survey)
```
##### - North America sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Geographic_group == "North_America"))
```

##### - Atlantic islands sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Geographic_group == "North_Atlantic_Isles"))
```

##### - Other European countries sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Geographic_group == "Other_Europe"))
```
##### - NO papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "0"))
```
##### - 1-5 papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "1-5"))
```

##### - 6-10 papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "6-10"))
```

##### - 11-20 papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "11-20"))
```

##### - 21+ papers sample

```{r}
summary(subset(x = Survey_Dummy_Variables_Rowan_Simple_V1, select = Confidence_in_survey, subset = Publications_read == "21+"))
```
### h) Papers : Who, how many ! 

```{r}
esquisser(Survey_Dummy_Variables_Rowan_Simple_V1)

library(ggplot2)

ggplot(Survey_Dummy_Variables_Rowan_Simple_V1) +
 aes(x = Publications_read) +
 geom_bar(fill = "#29467B") +
 labs(x = "Number of scientific publications regarding Arctic char read", y = "Counts") +
 theme_minimal() +
 scale_x_discrete(limits = c("0", "1-5", "6-10", "11-20", "21+"))
```
### i) Char interest 

```{r}
No_averaged_observations_char_interest <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  filter(., !between(Char_conservation_interest, 7.01, 7.99)) 

esquisser(No_averaged_observations_char_interest)

library(ggplot2)

ggplot(No_averaged_observations_char_interest) +
 aes(x = Char_conservation_interest) +
 geom_histogram(bins = 7L, 
 fill = "#112446") +
 theme_minimal()
```



- Char interest : I want a frequency/barplot.

- Sufficiently protected first and second: Compare means and variances.
  Maybe a plot with the two distributions.
- Anticipation of info: Just %. 
- Comfortable with experiment : Just % 
- Taxonomy : Just %. 
- TrueFalse Score : Average, min max. 

## Sufficiently protected

- Sufficiently protected first and second: Compare means and variances.
  Maybe a plot with the two distributions.
  
  The statement is "Arctic char populations in Ireland are sufficiently protected by general regulations on water quality so it is unnecessary to allocate more ressources to their conservation." 
```{r}
# Now a nice density plot
Sufficiently_protected_data_first <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  select(., Scale_Sufficently_Protected_First, name) %>% 
  rename("Scale_Sufficently_Protected" = Scale_Sufficently_Protected_First) %>% 
  mutate(., Time_asked = "First")
Sufficiently_protected_data_second <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
  select(., Scale_Sufficently_Protected_Second, name) %>% 
  rename("Scale_Sufficently_Protected" = Scale_Sufficently_Protected_Second) %>% 
  mutate(., Time_asked = "Second")

Sufficiently_protected_data <- rbind(Sufficiently_protected_data_first, Sufficiently_protected_data_second)

ggplot(Sufficiently_protected_data, aes(x = Scale_Sufficently_Protected, fill = Time_asked)) + geom_density(alpha = 0.5) +
 labs(x = "Disagreement scale with suggestion that char are sufficiently protected in Ireland") +
 theme_minimal()
```

```{r}
t.test(Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_First, Survey_Dummy_Variables_Rowan_Simple_V1$Scale_Sufficently_Protected_Second, alternative = "less" )
```
Try it on different subsets.
```{r}
Susbet_Students <- Survey_Dummy_Variables_Rowan_Simple_V1 %>% 
    filter(., Student_Dummy == 1)

t.test(Susbet_Students$Scale_Sufficently_Protected_First, Susbet_Students$Scale_Sufficently_Protected_Second, alternative = "less" )
```
## II) Extra models 


















































































































































































## END












This works